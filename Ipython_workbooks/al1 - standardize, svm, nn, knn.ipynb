{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# true, then pred\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import amyutility as p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'amyutility' from 'amyutility.pyc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43436, 23)\n",
      "(14479, 23)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traink = pd.read_csv('data/fromKen/full_train_2.csv')\n",
    "testk = pd.read_csv('data/fromKen/full_test_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43436, 56)\n",
      "(14479, 55)\n"
     ]
    }
   ],
   "source": [
    "print traink.shape\n",
    "print testk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg127</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>2024.00</td>\n",
       "      <td>nan</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg142</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>258</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2753.00</td>\n",
       "      <td>2751.00</td>\n",
       "      <td>2709.00</td>\n",
       "      <td>3528.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>10</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg104</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1668.00</td>\n",
       "      <td>2910.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg112</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>14</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>741.00</td>\n",
       "      <td>1107.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.00</td>\n",
       "      <td>F</td>\n",
       "      <td>reg106</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>131</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>359.00</td>\n",
       "      <td>325.00</td>\n",
       "      <td>531.00</td>\n",
       "      <td>654.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  Id\n",
       "0 29.00   M  reg127         1       N          2        Y        N        N     nan     nan 2024.00      nan     N    N     nan            0            0           0          0           0       0   1\n",
       "1 16.00   M  reg142         1       N        258        N        N        Y 2753.00 2751.00 2709.00  3528.00     N    N     nan           10          223           0         57           7       0   2\n",
       "2 22.00   M  reg104         1       N         28        N        N        Y     nan     nan 1668.00  2910.00     N    N     nan            6            6           2          1           0       0   3\n",
       "3 10.00   M  reg112         1       N         14        N        N        Y     nan     nan  741.00  1107.00     N    N     nan           13           13           0          2           1       0   4\n",
       "4 14.00   F  reg106         1       N        131        N        N        N  359.00  325.00  531.00   654.00     N    N     nan           14           57           0         16           1       0   5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'lapsed', u'age', u'sex', u'region', u'nregions', u'memtype', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ken's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'lapsed', u'age', u'sex', u'region', u'nregions', u'memtype', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor', u'age.na', u'r1.na', u'r2.na', u'r3.na', u'r.quick.na', u'r.intl.na', u'mon_less30', u'mon_31', u'mon_32', u'mon_33', u'mon_34', u'mon_35', u'mon_36', u'mon_37_60', u'mon_61_84', u'mon_85_120', u'mon_121_263', u'mon_264_plus',\n",
       "       u'games_0', u'games_1_5', u'games_6_10', u'games_11_20', u'games_21_34', u'games_35_49', u'games_50_plus', u'agesq', u'agecbd', u'allgames1yrsq', u'allgames1yrcbd', u'allgames5yrsq', u'allgames5yrcbd', u'memmonthssq', u'memmonthscbd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'age', u'sex', u'region', u'nregions', u'memtype', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor', u'age.na', u'r1.na', u'r2.na', u'r3.na', u'r.quick.na', u'r.intl.na', u'mon_less30', u'mon_31', u'mon_32', u'mon_33', u'mon_34', u'mon_35', u'mon_36', u'mon_37_60', u'mon_61_84', u'mon_85_120', u'mon_121_263', u'mon_264_plus',\n",
       "       u'games_0', u'games_1_5', u'games_6_10', u'games_11_20', u'games_21_34', u'games_35_49', u'games_50_plus', u'agesq', u'agecbd', u'allgames1yrsq', u'allgames1yrcbd', u'allgames5yrsq', u'allgames5yrcbd', u'memmonthssq', u'memmonthscbd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lapsed</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>11.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>61.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>16.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>47.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>11.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lapsed   age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  \\\n",
       "0      Y 11.00   M    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00   \n",
       "1      N 61.00   M    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83   \n",
       "2      Y 16.00   F    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20   \n",
       "3      Y 47.00   M    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00   \n",
       "4      Y 11.00   F    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69   \n",
       "\n",
       "   allgames5yrsq  allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0           0.00            0.00         5.99          8.99  \n",
       "1           6.80           10.20        10.59         15.88  \n",
       "2           6.80           10.20        10.53         15.79  \n",
       "3           0.00            0.00        11.19         16.78  \n",
       "4           7.17           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>2024.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>258</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2753.00</td>\n",
       "      <td>2751.00</td>\n",
       "      <td>2709.00</td>\n",
       "      <td>3528.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>10</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4.80</td>\n",
       "      <td>7.19</td>\n",
       "      <td>10.82</td>\n",
       "      <td>16.23</td>\n",
       "      <td>11.11</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1668.00</td>\n",
       "      <td>2910.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.27</td>\n",
       "      <td>9.41</td>\n",
       "      <td>3.89</td>\n",
       "      <td>5.84</td>\n",
       "      <td>3.89</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.73</td>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>14</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>741.00</td>\n",
       "      <td>1107.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.80</td>\n",
       "      <td>7.19</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.92</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.92</td>\n",
       "      <td>5.42</td>\n",
       "      <td>8.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>131</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>359.00</td>\n",
       "      <td>325.00</td>\n",
       "      <td>531.00</td>\n",
       "      <td>654.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.42</td>\n",
       "      <td>8.12</td>\n",
       "      <td>5.42</td>\n",
       "      <td>8.12</td>\n",
       "      <td>8.12</td>\n",
       "      <td>12.18</td>\n",
       "      <td>9.77</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 29.00   M    0.11         1       N          2        Y        N        N 1942.12 1811.61 2024.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      0           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   6.80   10.20           0.00            0.00           0.00   \n",
       "1 16.00   M    0.02         1       N        258        N        N        Y 2753.00 2751.00 2709.00  3528.00     N    N 3477.56           10          223           0         57           7       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False       True       False       False       False          True   5.67    8.50           4.80            7.19          10.82   \n",
       "2 22.00   M    0.00         1       N         28        N        N        Y 1942.12 1811.61 1668.00  2910.00     N    N 3477.56            6            6           2          1           0       0       0      1      1      0           0          1       True  False  False  False  False  False  False     False     False      False       False        False   False     False       True       False       False       False         False   6.27    9.41           3.89            5.84           3.89   \n",
       "3 10.00   M    0.12         1       N         14        N        N        Y 1942.12 1811.61  741.00  1107.00     N    N 3477.56           13           13           0          2           1       0       0      1      1      0           0          1       True  False  False  False  False  False  False     False     False      False       False        False   False     False      False        True       False       False         False   4.80    7.19           5.28            7.92           5.28   \n",
       "4 14.00   F    0.04         1       N        131        N        N        N  359.00  325.00  531.00   654.00     N    N 3477.56           14           57           0         16           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False        True       False       False          True   5.42    8.12           5.42            8.12           8.12   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0            0.00         2.20          3.30  \n",
       "1           16.23        11.11         16.67  \n",
       "2            5.84         6.73         10.10  \n",
       "3            7.92         5.42          8.12  \n",
       "4           12.18         9.77         14.65  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traink_y = traink[['lapsed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lapsed\n",
       "0      Y\n",
       "1      N\n",
       "2      Y\n",
       "3      Y\n",
       "4      Y"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traink_x = traink.drop('lapsed', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43436, 55)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00   M    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00   M    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00   F    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00   M    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00   F    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0            0.00         5.99          8.99  \n",
       "1           10.20        10.59         15.88  \n",
       "2           10.20        10.53         15.79  \n",
       "3            0.00        11.19         16.78  \n",
       "4           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = (traink_y.lapsed.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key train_y\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 14477, 14478, 14479])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key test_ids: for writing to predictions\n",
    "test_ids = test.Id.values\n",
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# key df_all: combine test and train into df_all, test_idx \n",
    "test_idx = traink_x.shape[0]\n",
    "df_all = pd.concat((traink_x, testk), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57915, 55)\n",
      "43436\n"
     ]
    }
   ],
   "source": [
    "print df_all.shape\n",
    "print test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key: \n",
    "- df_all\n",
    "- test_idx\n",
    "- train_y\n",
    "- test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Types and Convert\n",
    "\n",
    "- need to convert sex, memtype, mem_mag1, mem_mag2, hasemail, extra, intl\n",
    "- Can leave bools alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONVERT = ['sex', 'memtype', 'mem_mag1', 'mem_mag2', 'hasemail', 'extra', 'intl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sex\n",
    "\n",
    "- males 0\n",
    "- females 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFeCAYAAACvnuTEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKABAcf4PyfuyvtiPZrgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d3b3250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mostly male (males 0, females 1)\n",
    "gender = df_all.groupby('sex').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "gender.plot(kind='barh', title = 'Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00   M    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00   M    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00   F    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00   M    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00   F    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0            0.00         5.99          8.99  \n",
       "1           10.20        10.59         15.88  \n",
       "2           10.20        10.53         15.79  \n",
       "3            0.00        11.19         16.78  \n",
       "4           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               float64\n",
       "sex                object\n",
       "region            float64\n",
       "nregions            int64\n",
       "memtype            object\n",
       "memmonths           int64\n",
       "mem_mag1           object\n",
       "mem_mag2           object\n",
       "hasemail           object\n",
       "r1                float64\n",
       "r2                float64\n",
       "r3                float64\n",
       "r.quick           float64\n",
       "extra              object\n",
       "intl               object\n",
       "r.intl            float64\n",
       "allgames1yr         int64\n",
       "allgames5yr         int64\n",
       "fastevents          int64\n",
       "medevents           int64\n",
       "slowevents          int64\n",
       "nfloor              int64\n",
       "age.na              int64\n",
       "r1.na               int64\n",
       "r2.na               int64\n",
       "r3.na               int64\n",
       "r.quick.na          int64\n",
       "r.intl.na           int64\n",
       "mon_less30           bool\n",
       "mon_31               bool\n",
       "mon_32               bool\n",
       "mon_33               bool\n",
       "mon_34               bool\n",
       "mon_35               bool\n",
       "mon_36               bool\n",
       "mon_37_60            bool\n",
       "mon_61_84            bool\n",
       "mon_85_120           bool\n",
       "mon_121_263          bool\n",
       "mon_264_plus         bool\n",
       "games_0              bool\n",
       "games_1_5            bool\n",
       "games_6_10           bool\n",
       "games_11_20          bool\n",
       "games_21_34          bool\n",
       "games_35_49          bool\n",
       "games_50_plus        bool\n",
       "agesq             float64\n",
       "agecbd            float64\n",
       "allgames1yrsq     float64\n",
       "allgames1yrcbd    float64\n",
       "allgames5yrsq     float64\n",
       "allgames5yrcbd    float64\n",
       "memmonthssq       float64\n",
       "memmonthscbd      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 M\n",
       "1                 M\n",
       "2                 F\n",
       "3                 M\n",
       "4                 F\n",
       "5                 F\n",
       "6                 M\n",
       "7                 M\n",
       "8                 M\n",
       "9                 M\n",
       "10                M\n",
       "11                M\n",
       "12                M\n",
       "13                M\n",
       "14                M\n",
       "15                F\n",
       "16                M\n",
       "17                M\n",
       "18                F\n",
       "19                M\n",
       "20                M\n",
       "21                F\n",
       "22                M\n",
       "23                M\n",
       "24                M\n",
       "25                M\n",
       "26                M\n",
       "27                M\n",
       "28                F\n",
       "29                M\n",
       "30                M\n",
       "31                M\n",
       "32                M\n",
       "33                F\n",
       "34                M\n",
       "35                M\n",
       "36                M\n",
       "37                M\n",
       "38                M\n",
       "39                F\n",
       "40                M\n",
       "41                M\n",
       "42                M\n",
       "43                F\n",
       "44                M\n",
       "45                M\n",
       "46                M\n",
       "47                M\n",
       "48                M\n",
       "49                M\n",
       "50                M\n",
       "51                M\n",
       "52                F\n",
       "53                M\n",
       "54                M\n",
       "55                F\n",
       "56                M\n",
       "57                M\n",
       "58                M\n",
       "59                M\n",
       "60                M\n",
       "61       Z_dummy_NA\n",
       "62                M\n",
       "63                M\n",
       "64                M\n",
       "65                M\n",
       "66                M\n",
       "67                M\n",
       "68                M\n",
       "69                M\n",
       "70                M\n",
       "71                M\n",
       "72                M\n",
       "73                M\n",
       "74                M\n",
       "75                M\n",
       "76                M\n",
       "77                M\n",
       "78                M\n",
       "79                M\n",
       "80       Z_dummy_NA\n",
       "81                M\n",
       "82                F\n",
       "83                M\n",
       "84                M\n",
       "85       Z_dummy_NA\n",
       "86                F\n",
       "87                M\n",
       "88                M\n",
       "89                M\n",
       "90                M\n",
       "91                M\n",
       "92                M\n",
       "93                M\n",
       "94                M\n",
       "95                M\n",
       "96                M\n",
       "97                M\n",
       "98                M\n",
       "99                M\n",
       "            ...    \n",
       "14379             M\n",
       "14380             M\n",
       "14381             M\n",
       "14382             M\n",
       "14383             M\n",
       "14384             F\n",
       "14385             M\n",
       "14386             M\n",
       "14387             M\n",
       "14388             M\n",
       "14389             M\n",
       "14390             M\n",
       "14391             M\n",
       "14392             M\n",
       "14393             M\n",
       "14394             M\n",
       "14395             M\n",
       "14396             M\n",
       "14397             M\n",
       "14398             M\n",
       "14399             M\n",
       "14400             M\n",
       "14401             M\n",
       "14402             M\n",
       "14403             M\n",
       "14404             M\n",
       "14405             M\n",
       "14406             F\n",
       "14407             M\n",
       "14408             M\n",
       "14409             F\n",
       "14410             M\n",
       "14411             M\n",
       "14412             M\n",
       "14413             M\n",
       "14414             M\n",
       "14415             M\n",
       "14416             M\n",
       "14417             M\n",
       "14418             M\n",
       "14419             M\n",
       "14420             M\n",
       "14421             F\n",
       "14422             M\n",
       "14423             M\n",
       "14424    Z_dummy_NA\n",
       "14425             M\n",
       "14426             F\n",
       "14427             M\n",
       "14428             M\n",
       "14429             M\n",
       "14430             M\n",
       "14431             F\n",
       "14432             M\n",
       "14433             M\n",
       "14434             M\n",
       "14435             M\n",
       "14436             M\n",
       "14437             M\n",
       "14438             M\n",
       "14439    Z_dummy_NA\n",
       "14440             M\n",
       "14441             M\n",
       "14442             M\n",
       "14443             M\n",
       "14444             M\n",
       "14445             M\n",
       "14446             M\n",
       "14447             M\n",
       "14448             M\n",
       "14449             F\n",
       "14450             M\n",
       "14451             M\n",
       "14452             M\n",
       "14453             M\n",
       "14454             M\n",
       "14455             M\n",
       "14456             M\n",
       "14457             F\n",
       "14458             M\n",
       "14459             M\n",
       "14460             M\n",
       "14461             M\n",
       "14462             M\n",
       "14463             M\n",
       "14464             M\n",
       "14465             M\n",
       "14466             F\n",
       "14467             M\n",
       "14468             M\n",
       "14469             F\n",
       "14470             M\n",
       "14471             M\n",
       "14472             M\n",
       "14473             F\n",
       "14474    Z_dummy_NA\n",
       "14475             F\n",
       "14476             M\n",
       "14477             M\n",
       "14478             M\n",
       "Name: sex, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.sex = (df_all.sex.values=='F')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  \\\n",
       "0 11.00    0    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00   \n",
       "1 61.00    0    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83   \n",
       "2 16.00    1    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20   \n",
       "3 47.00    0    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00   \n",
       "4 11.00    1    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69   \n",
       "\n",
       "   allgames5yrsq  allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0           0.00            0.00         5.99          8.99  \n",
       "1           6.80           10.20        10.59         15.88  \n",
       "2           6.80           10.20        10.53         15.79  \n",
       "3           0.00            0.00        11.19         16.78  \n",
       "4           7.17           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memtype\n",
    "- make Normal the reference category\n",
    "- memtypeA=1 for affiliate\n",
    "- memtypeF=1 for family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1061625d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# membership types A, F, N \n",
    "memtype = df_all.groupby('memtype').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "memtype.plot(kind='barh', title = 'Memtype')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['memtypeA'] = (df_all.memtype=='A')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['memtypeF'] = (df_all.memtype=='F')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop('memtype', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mem_mag1 & mem_mag2 & hasemail\n",
    "\n",
    "- only yes or no... convert yes to 1, no to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGAM8QYAwBjiDQCAMcQbAABj/g8VjRY/U6OGdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d2e2490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memmag1 = df_all.groupby('mem_mag1').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "memmag1.plot(kind='barh', title = 'mem_mag1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFeCAYAAAB+T51FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dd5c150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memmag2 = df_all.groupby('mem_mag2').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "memmag2.plot(kind='barh', title = 'mem_mag2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFhFJREFUeJzt3X9UlvX9x/HXzY0oBYLHgZ6kHwgyKDzVIg6tQyMWJSc9\nptmizDPLWpoKsWMHIddUHKItO9WpJeucmehqa9kPd3Y6lW4uySO0M93Q4DTFMX9C5k9Q4se1Pzry\nze8Y3jov7vt99Xz81X1f4PX+eAvPrvu67vv2OY7jCAAAmBEW7AEAAMD5Id4AABhDvAEAMIZ4AwBg\nDPEGAMAY4g0AgDHEGzCktrZWEyZMCPYY5/T888/rnXfekSSlpqbq6NGjQZ4I8JbwYA8AwHsKCwt7\n/9vn8wVxEsCbiDdgTFtbm3784x9r9+7d+vLLL1VeXq7hw4dr8eLFam9vV0tLi9LS0vTss88qIiJC\nzz//vDZs2KBBgwYpNjZWlZWV+ta3vqVdu3apoqJCR48eVU9Pj6ZNm6bJkyertrZWK1asUHx8vD77\n7DNFRkZq7ty5qq6u1p49e5SXl6fS0lI5jqOKigr97W9/U1tbmxzH0ZIlS3T99dertLRUKSkpevDB\nB8X7QAEXH/EGjGlpadGDDz6osWPHatWqVXrhhRd0zTXXaNKkSZowYYK6uro0efJkbdq0SWPHjtXq\n1au1ZcsWDRo0SKtWrdL27duVk5OjoqIiPf3000pLS9PJkyd17733Kjk5WZJUX1+v3/3ud0pNTdUj\njzyiqqoqrVmzRsePH1d2drYefvhh7du3T62trfrNb34jSaqqqlJVVZV+8YtfBPOvB/hGIN6AMZdf\nfrnGjh0rSUpLS9O6dev0xBNPaPPmzXrllVe0Z88etba2qq2tTSNGjFBaWpomTZqk7Oxs3XLLLbrp\nppu0a9cuNTc3q6ysrPfIuKOjQzt37tTo0aM1atQopaamSpKuuOIKRUdHy+/3a9iwYYqKitKxY8d0\n3XXXqaioSK+99pqam5tVW1urqKiooP29AN8kxBswJjz8/35sfT6fHMdRcXGxuru7lZ+fr1tvvVUH\nDhzo3V5dXa36+np9/PHHWrp0qbKysjRlyhQNHTpUb731Vu+fdfjwYUVHR2vbtm2KiIj4r/s8409/\n+pMqKir00EMP6bbbbtPo0aO1fv16l1YN4Ou42hzwgJqaGs2ePVv5+flyHEfbt29Xd3e3GhoaNH78\neCUlJelHP/qRpk+froaGBiUmJmrw4MF69913JUkHDhzQ+PHjtWPHjoD3+fHHHys3N1cFBQVKT0/X\nhg0b1NPT49YSAXwNR96ABxQXF2v27NmKjY1VZGSkMjMz1dzcrLvvvlv5+fmaPHmyLrnkEkVGRmrB\nggUaNGiQXnrpJS1ZskSvvPKKuru7VVxcrOuvv161tbX97uvM1eMFBQWaN2+eJk6cKL/fr4yMDL3/\n/vv/9esBXDw+PhIUAABbeNocAABjiDcAAMYQbwAAjAmpC9ZOnz6t+vp6xcXFye/3B3scAABc193d\nrdbWVqWnp2vIkCEBfU9Ixbu+vl5Tp04N9hgAAAy4tWvXKiMjI6CvDal4x8XFSfpqASNHjgzyNAAA\nuO/gwYOaOnVqbwMDEVLxPvNU+ciRI5WQkBDkaQAAGDjnc7qYC9YAADCGeAMAYAzxBgDAGOINAIAx\nxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADGEG8AAIwh\n3gAAGEO8AQAwhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAAjCHeAAAYQ7wBADCGeAMAYAzx\nBgDAGOINAIAxxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgTHiwB+hLU1OT2tvbgz0GAMDD\nkpKS5Pf7gz3GBQnJeOfmSl1dwZ4CAOBdTWpslFJSUoI9yAUJyXhLiZISgj0EAAAhiXPeAAAYQ7wB\nADCGeAMAYAzxBgDAGOINAIAxxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0A\ngDHEGwAAY4g3AADGEG8AAIwh3gAAGEO8AQAwhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAA\njCHeAAAYQ7wBADCGeAMAYAzxBgDAGOINAIAxxBsAAGOINwAAxhBvAACMcT3ehYWFqqqq6r3d1tam\ncePGqbGx0e1dAwDgSa7He9GiRXr99de1a9cuSdLy5ctVUFCgb3/7227vGgAAT3I93sOGDdNTTz2l\nJ598UrW1tdq7d6+mT5/u9m4BAPCsATnnnZOTo9GjR6usrEyVlZUDsUsAADwrfKB2dNddd6mjo0Nx\ncXEDtUsAADyJq80BADCGeAMAYMyAPW2emZmpzMzMgdodAACexZE3AADGEG8AAIwh3gAAGEO8AQAw\nhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAAjCHeAAAYQ7wBADCGeAMAYAzxBgDAGOINAIAx\nxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADGEG8AAIwh\n3gAAGEO8AQAwhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAAjAkP9gB9a5LUHuwhAACe1SQp\nMdhDXLCQjPfGjdKIEcGeAgDgXYlKSkoK9hAXLCTjnZiYqISEhGCPAQBASOKcNwAAxhBvAACMId4A\nABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADGEG8AAIwh3gAAGEO8AQAwhngDAGAM8QYA\nwBjiDQCAMcQbAABjiDcAAMaE97fx7bff7veb77rrros6DAAAOLd+471169Z+v5l4AwAw8PqN99Kl\nSwdqDgAAEKB+4/3oo49q5cqVys3Nlc/n+4/tGzZscG0wAADQt37jXV5eLkmqrq4ekGEAAMC59Rvv\n+Ph4SVJcXJw2bdqktrY2SVJ3d7f27t2roqIi9ycEAABn6TfeZ8yZM0enTp1Sc3OzMjIyVFdXp+uu\nu87t2QAAQB8Cep13U1OTVq9erby8PD388MN644031NLS4vZsAACgDwHFe/jw4fL5fEpMTFRjY6NG\njBihL7/80u3ZAABAHwJ62nzMmDEqLy/Xfffdp3nz5qmlpUWdnZ1uzwYAAPoQ0JH3woULlZ+fr+Tk\nZBUWFqqlpUXPPPOM27MBAIA+BBRvv9+v6Oho1dXVKTo6WnfccYeOHTvm9mwAAKAPAT1tXlxcrJ07\nd/a+dEySfD6fVq9e7dpgAACgbwHFu6GhQX/4wx/k9/vdngcAAJxDQE+bX3vttfrnP//p9iwAACAA\nAR15Z2Vlafz48YqPj5ff75fjOPL5fLy3OQAAQRBQvJ977jm9+uqruuyyy9yeBwAAnENA8R42bJgy\nMjL6/GQxAAAwsAKKd2pqqn7wgx/ou9/9rgYNGtR7/5w5c1wbDAAA9C2geF922WU8ZQ4AQIgI+FPF\n2tvb1dzcrJSUFJ0+fVqXXHKJ27MBAIA+BPRSsS1btmjixIl67LHH9Pnnnys3N1ebN292ezYAANCH\ngOK9YsUK/frXv9bQoUMVHx+vNWvWaPny5W7PBgAA+hBQvHt6ehQXF9d7Ozk52bWBAABA/wI65z1y\n5Ej98Y9/lM/n0/Hjx7V27VouYAMAIEgCOvJevHix1q9frwMHDigvL0+ffvqpFi9e7PZsAACgDwEd\neQ8fPlwrVqyQJJ04cUIHDx486xPGAADAwAnoyPuNN95QaWmpvvjiC915550qLCzUs88+6/ZsAACg\nDwHF+7XXXlNJSYl+//vf6/vf/77Wr1+vjz76yO3ZAABAHwKKtyTFxsZq06ZNysnJUXh4uDo6Otyc\nCwAA/BcBxTs5OVmPPvqo9u7dq5tuuklFRUVKT093ezYAANCHgC5Yq6io0F//+leNGTNGERERmjhx\nor73ve+5PRsAAOhDQPE+duyYduzYodraWjmOo56eHr333nu8yxoAAEEQ0NPmc+bM0aeffqp3331X\np06d0saNGxUWFvDpcgAAcBEFVOAjR45o2bJlys3N1e23367q6mp99tlnbs8GAAD6EFC8Y2JiJEmJ\niYlqaGhQdHS0Ojs7XR0MAAD0LaBz3llZWSosLFRJSYkeeugh7dixQ5GRkW7PBgAA+hBQvGfPnq3X\nX39ddXV1KigokM/n06hRo9yeDQAA9CGgeD/++ONqbW1VUlKSfD6f2zMBAIB+BBTv3bt367333nN7\nFgAAEICALli74oortH//frdnAQAAAej3yHvatGny+Xz64osvNGHCBKWmpsrv9/duX716tesDAgCA\ns/Ub77lz5w7UHAAAIED9xjszM3Og5gAAAAHiPU4BADCGeAMAYAzxBgDAGOINAIAxxBsAAGOINwAA\nxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY/r9VLFgaWpqUnt7e7DHgHFJSUln\nff48AHhFSMY7N1fq6gr2FLCtSY2NUkpKSrAHAYCLLiTjLSVKSgj2EAAAhCTOeQMAYAzxBgDAGOIN\nAIAxxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADGEG8A\nAIwh3gAAGEO8AQAwhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAAjCHeAAAYQ7wBADCGeAMA\nYAzxBgDAGOINAIAxxBsAAGOINwAAxhBvAACMId4AABhDvAEAMMbVeNfW1iojI0OHDh3qve+ZZ57R\n22+/7eZuAQDwNNePvCMiIlRaWur2bgAA+MZwPd5ZWVmKiYnR2rVr3d4VAADfCK7H2+fzaeHChXr1\n1VfV3Nzs9u4AAPC8AblgLSYmRqWlpSopKZHjOAOxSwAAPGvArja/9dZblZiYqHXr1g3ULgEA8KQB\nfalYWVmZhgwZMpC7BADAc8Ld/MMzMzOVmZnZezsqKkobN250c5cAAHgeb9ICAIAxxBsAAGOINwAA\nxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADGEG8AAIwh3gAAGEO8AQAw\nhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAAjCHeAAAYQ7wBADCGeAMAYAzxBgDAGOINAIAx\nxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHhwR6gb02S2oM9BExrkpQY\n7CEAwBUhGe+NG6URI4I9BWxLVFJSUrCHAABXhGS8ExMTlZCQEOwxAAAISZzzBgDAGOINAIAxxBsA\nAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADGEG8AAIwh3gAA\nGEO8AQAwhngDAGAM8QYAwBjiDQCAMcQbAABjiDcAAMYQbwAAjCHeAAAYQ7wBADCGeAMAYAzxBgDA\nGOINAIAxxBsAAGOINwAAxhBvAACMId4AABhDvAEAMIZ4AwBgDPEGAMAY4g0AgDHEGwAAY4g3AADG\nhAd7gK/r7u6WJB08eDDIkwAAMDDONO9MAwMRUvFubW2VJE2dOjXIkwAAMLBaW1t15ZVXBvS1Psdx\nHJfnCdjp06dVX1+vuLg4+f3+YI8DAIDruru71draqvT0dA0ZMiSg7wmpeAMAgHPjgjUAAIwh3gAA\nGEO8AQAwhngDAGBMyLxUzHEcLVy4UI2NjYqIiNDPfvYzXX755cEe66LYvn27fv7zn6u6ulrNzc2a\nP3++wsLCNGbMGP30pz8N9ngXrKurS2VlZdq3b586Ozs1c+ZMJScne2Z9PT09WrBggZqamhQWFqZF\nixYpIiLCM+uTpMOHD+vuu+/Wr371K/n9fk+tbfLkyYqKipIkJSQkaObMmZ5aX1VVlTZu3KjOzk7d\nf//9uvHGGz2zvrfeekvr1q2Tz+dTR0eHGhoatHbtWlVUVHhifV1dXSopKdG+ffsUHh6u8vLy8//5\nc0LE+++/78yfP99xHMfZtm2bM2vWrCBPdHH88pe/dMaPH+/ce++9juM4zsyZM526ujrHcRznqaee\ncj744INgjvc/efPNN52KigrHcRzn2LFjTk5OjqfW98EHHzhlZWWO4zjO1q1bnVmzZnlqfZ2dnc7s\n2bOdO+64w9m9e7en1tbR0eFMmjTprPu8tL6tW7c6M2fOdBzHcdra2pwXXnjBU+v7ukWLFjm//e1v\nPbW+Dz/80Hn88ccdx3GcmpoaZ+7cuee9vpB52vwvf/mLsrOzJUnXXnut6uvrgzzRxXHllVfqxRdf\n7L29Y8cOZWRkSJJuueUWbdmyJVij/c/y8/NVVFQk6avXKfr9fu3cudMz67vttttUXl4uSdq/f79i\nYmI8tb5ly5bpvvvuU3x8vBzH8dTaGhoa1N7erhkzZmj69Onavn27p9a3efNmpaSk6LHHHtOsWbOU\nk5PjqfWd8fe//13/+Mc/dM8993jqd+dVV12l7u5uOY6jEydOKDw8/Lwfv5B52vzkyZOKjo7uvR0e\nHq6enh6FhYXM/19ckLy8PO3bt6/3tvO1l9VfeumlOnHiRDDGuigiIyMlffXYFRUVqbi4WMuWLevd\nbn19khQWFqb58+frww8/1HPPPaeamprebZbXt27dOg0fPlw333yzXn75ZUlfnSY4w/LaJGnIkCGa\nMWOG7rnnHu3Zs0ePPPKIp372jhw5ov3792vlypX617/+pVmzZnnq8TujqqpKc+fO/Y/7ra/v0ksv\n1d69ezVu3DgdPXpUL7/8sj755JOztp9rfSET76ioKLW1tfXe9kK4+/L1NbW1tWno0KFBnOZ/d+DA\nAc2ZM0cPPPCA7rzzTj399NO927ywPkmqrKzU4cOHNWXKFHV0dPTeb3l9Z84n1tTUqLGxUSUlJTpy\n5Ejvdstrk746sjnzNpNXXXWVYmNjtXPnzt7t1tcXGxurpKQkhYeHKzExUYMHD9ahQ4d6t1tfnySd\nOHFCe/bs0Y033ijJW787V61apezsbBUXF+vQoUOaNm2aOjs7e7cHsr6QqeN3vvMdbdq0SZK0bds2\npaSkBHkid1x99dWqq6uTJP35z3/WDTfcEOSJLtznn3+uGTNm6IknntCkSZMkSWlpaZ5Z3zvvvKOq\nqipJ0uDBgxUWFqb09HTV1tZKsr2+NWvWqLq6WtXV1UpNTdXy5cuVnZ3tmcfuzTffVGVlpSTp0KFD\nOnnypG6++WZPPHaSdMMNN+ijjz6S9NX6Tp06paysLM+sT5Lq6uqUlZXVe9tLv1tiYmJ6L6aMjo5W\nV1eXrr766vN6/ELmyDsvL081NTUqKCiQJC1dujTIE7mjpKREP/nJT9TZ2amkpCSNGzcu2CNdsJUr\nV+r48eN66aWX9OKLL8rn8+nJJ5/UkiVLPLG+22+/XaWlpXrggQfU1dWlBQsWaPTo0VqwYIEn1vf/\neenf5pQpU1RaWqr7779fYWFhqqysVGxsrGceu5ycHH3yySeaMmVK7yt1Ro0a5Zn1SVJTU9NZrzjy\n0r/PH/7whyorK9PUqVPV1dWlefPm6Zprrjmvx4/3NgcAwJiQedocAAAEhngDAGAM8QYAwBjiDQCA\nMcQbAABjiDcAAMYQbwAAjCHeAAAY828pa7jssE1jJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108789890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasemail = df_all.groupby('hasemail').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "hasemail.plot(kind='barh', title = 'hasemail')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.mem_mag1 = (df_all.mem_mag1.values=='Y')*1\n",
    "df_all.mem_mag2 = (df_all.mem_mag2.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.hasemail = (df_all.hasemail.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00    0    0.12         1         19         0         0         0 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00    0    0.12         1        198         1         0         1 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00    1    0.12         1        192         0         0         1  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00    0    0.12         1        268         1         0         1 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00    1    0.12         1        101         0         0         0  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  \n",
       "0            0.00         5.99          8.99         0         0  \n",
       "1           10.20        10.59         15.88         0         0  \n",
       "2           10.20        10.53         15.79         0         0  \n",
       "3            0.00        11.19         16.78         0         0  \n",
       "4           10.75         9.25         13.87         0         0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra, intl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFeCAYAAAB+T51FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108794310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extra = df_all.groupby('extra').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "extra.plot(kind='barh', title = 'extra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3gBQjHgDQDHiDQDFiDcAFPM/+ic1i4HfanwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102b3a510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intl = df_all.groupby('intl').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "intl.plot(kind='barh', title = 'intl')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.extra = (df_all.extra.values=='Y')*1\n",
    "df_all.intl = (df_all.intl.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail      r1      r2      r3  r.quick  extra  intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00    0    0.12         1         19         0         0         0 1942.12 1811.61 1557.56  2007.74      0     0 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00    0    0.12         1        198         1         0         1 2178.00 2215.00 2291.00  2932.00      1     0 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00    1    0.12         1        192         0         0         1  627.00  628.00 1362.00  2007.00      0     0 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00    0    0.12         1        268         1         0         1 2600.00 2601.00 2602.00  2007.74      0     0 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00    1    0.12         1        101         0         0         0  464.00  466.00  958.00  1356.00      0     0 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  \n",
       "0            0.00         5.99          8.99         0         0  \n",
       "1           10.20        10.59         15.88         0         0  \n",
       "2           10.20        10.53         15.79         0         0  \n",
       "3            0.00        11.19         16.78         0         0  \n",
       "4           10.75         9.25         13.87         0         0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO region is discrete?? \n",
    "# last 8 are sq and cubed terms \n",
    "STANDARDIZE = ['age', 'region', 'nregions', 'memmonths', 'r1', 'r2', 'r3', 'r.quick', 'r.intl', \n",
    "               'allgames1yr', 'allgames5yr', 'fastevents', 'medevents', 'slowevents', 'nfloor', \n",
    "              'agesq', 'agecbd', 'allgames1yrsq', 'allgames1yrcbd', 'allgames5yrsq', 'allgames5yrcbd', 'memmonthssq', \n",
    "               'memmonthscbd']\n",
    "\n",
    "\n",
    "INDICATORS = ['sex', 'mem_mag1', 'mem_mag2', 'hasemail', 'extra', 'intl', 'age.na', 'r1.na', 'r2.na', \n",
    "             'r3.na', 'r.quick.na', 'r.intl.na', 'mon_less30', 'mon_31', 'mon_32', 'mon_33', 'mon_34', \n",
    "             'mon_35', 'mon_36', 'mon_37_60', 'mon_61_84', 'mon_85_120', 'mon_121_263', 'mon_264_plus', \n",
    "             'games_0', 'games_1_5', 'games_6_10', 'games_11_20', 'games_21_34', 'games_35_49', 'games_50_plus', \n",
    "             'memtypeA', 'memtypeF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STANDARDIZE) + len(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_scale = preprocessing.StandardScaler().fit(df_all[STANDARDIZE])\n",
    "std = std_scale.transform(df_all[STANDARDIZE])\n",
    "df_all[STANDARDIZE] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.20</td>\n",
       "      <td>0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.12</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.24</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail    r1    r2    r3  r.quick  extra  intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 -0.62    0    1.58     -0.18      -0.71         0         0         0 -0.01 -0.00 -0.00    -0.00      0     0   -0.00        -0.62        -0.52       -0.16      -0.46       -0.34   -0.07       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False  -0.63   -0.63          -1.36           -1.36          -1.57   \n",
       "1  1.91    0    1.58     -0.18       0.52         1         0         1  0.35  0.53  0.79     0.79      1     0   -0.00        -0.45        -0.19        0.00      -0.46        0.50   -0.07       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   1.74    1.74          -0.19           -0.19           0.45   \n",
       "2 -0.37    1    1.58     -0.18       0.48         0         0         1 -2.00 -1.58 -0.21    -0.00      0     0   -0.00         0.65        -0.19       -0.16      -0.22       -0.26   -0.07       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True  -0.13   -0.13           1.11            1.11           0.45   \n",
       "3  1.20    0    1.58     -0.18       1.00         1         0         1  0.99  1.05  1.12    -0.00      0     0   -0.00        -0.62        -0.52       -0.16      -0.46       -0.34   -0.07       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   1.37    1.37          -1.36           -1.36          -1.57   \n",
       "4 -0.62    1    1.58     -0.18      -0.15         0         0         0 -2.24 -1.79 -0.65    -0.56      0     0   -0.00        -0.10        -0.13       -0.16       0.03       -0.34   -0.07       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True  -0.63   -0.63           0.50            0.50           0.56   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  \n",
       "0           -1.57        -0.78         -0.78         0         0  \n",
       "1            0.45         0.92          0.92         0         0  \n",
       "2            0.45         0.90          0.90         0         0  \n",
       "3           -1.57         1.14          1.14         0         0  \n",
       "4            0.56         0.43          0.43         0         0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Females\n",
    "importance_list = clfForest.feature_importances_\n",
    "name_list = all_features\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "# just get top (in reverse order)\n",
    "top_imp = importance_list[-20:]\n",
    "top_names = name_list[-20:]\n",
    "plt.barh(range(len(top_names)),top_imp,align='center')\n",
    "plt.yticks(range(len(top_names)),top_names)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Top 20 Features for Females')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df_all\n",
    "test_idx\n",
    "train_y\n",
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ GRIDSEARCH\n",
      "BEST {'penalty': 'l1', 'C': 1.0} -0.549837084809\n",
      "SCORES:  [mean: -0.69315, std: 0.00000, params: {'penalty': 'l1', 'C': 1e-05}, mean: -0.65068, std: 0.00118, params: {'penalty': 'l2', 'C': 1e-05}, mean: -0.69315, std: 0.00000, params: {'penalty': 'l1', 'C': 0.0001}, mean: -0.60274, std: 0.00316, params: {'penalty': 'l2', 'C': 0.0001}, mean: -0.60157, std: 0.00308, params: {'penalty': 'l1', 'C': 0.001}, mean: -0.57350, std: 0.00419, params: {'penalty': 'l2', 'C': 0.001}, mean: -0.56092, std: 0.00448, params: {'penalty': 'l1', 'C': 0.01}, mean: -0.55408, std: 0.00446, params: {'penalty': 'l2', 'C': 0.01}, mean: -0.55034, std: 0.00438, params: {'penalty': 'l1', 'C': 0.1}, mean: -0.54996, std: 0.00431, params: {'penalty': 'l2', 'C': 0.1}, mean: -0.54984, std: 0.00431, params: {'penalty': 'l1', 'C': 1.0}, mean: -0.54985, std: 0.00427, params: {'penalty': 'l2', 'C': 1.0}, mean: -0.54989, std: 0.00427, params: {'penalty': 'l1', 'C': 10.0}, mean: -0.54989, std: 0.00427, params: {'penalty': 'l2', 'C': 10.0}, mean: -0.54990, std: 0.00427, params: {'penalty': 'l1', 'C': 100.0}, mean: -0.54989, std: 0.00427, params: {'penalty': 'l2', 'C': 100.0}]\n",
      "############\n",
      "############\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.54815\n",
      "Test set error = 0.54840\n",
      "----------\n",
      "############\n",
      "107.3 seconds runtime\n"
     ]
    }
   ],
   "source": [
    "# KAGGLE: 0.55058 \n",
    "# test set  0.54623\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "modellr = LogisticRegression()\n",
    "\n",
    "# parameters\n",
    "penalties = ['l1', 'l2']\n",
    "cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0] # default = 1.0\n",
    "# cs = [0.1, 1]\n",
    "params = {'penalty': penalties, 'C': cs}\n",
    "    \n",
    "predlr = p2.get_pred(modellr, df_all, train_y, 'LR_Ken_asis', track_dict=track_dict, test_idx=test_idx, train_size=0.8, \n",
    "                     columns=None, parameters=params, score_func='log_loss', n_folds=5, predict=True)\n",
    "\n",
    "print '%0.1f seconds runtime' % (time.time() - start)\n",
    "\n",
    "# write predictions\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST {'penalty': 'l1', 'C': 1} -0.547849871031 [mean: -0.54851, std: 0.00653, params: {'penalty': 'l1', 'C': 0.1}, \n",
    "                                                mean: -0.54915, std: 0.00701, params: {'penalty': 'l2', 'C': 0.1}, \n",
    "                                                mean: -0.54785, std: 0.00644, params: {'penalty': 'l1', 'C': 1}, \n",
    "                                                mean: -0.54838, std: 0.00682, params: {'penalty': 'l2', 'C': 1}]\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'LR_Ken_asis': [LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
    "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "            verbose=0, warm_start=False),\n",
    "  0.54792442961193033,\n",
    "  0.54938381530612657]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_file('predictions/LR_Ken_asis.csv', predlr[:, 1], test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ GRIDSEARCH\n",
      "BEST {'max_features': 0.5, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25} -0.544138029184\n",
      "SCORES:  [mean: -0.62463, std: 0.02499, params: {'max_features': 0.3, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.57475, std: 0.00597, params: {'max_features': 0.3, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.56886, std: 0.00849, params: {'max_features': 0.3, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.55250, std: 0.00231, params: {'max_features': 0.3, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54956, std: 0.00327, params: {'max_features': 0.3, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54881, std: 0.00327, params: {'max_features': 0.3, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54775, std: 0.00232, params: {'max_features': 0.3, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54613, std: 0.00270, params: {'max_features': 0.3, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54602, std: 0.00291, params: {'max_features': 0.3, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54557, std: 0.00215, params: {'max_features': 0.3, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54518, std: 0.00228, params: {'max_features': 0.3, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54465, std: 0.00223, params: {'max_features': 0.3, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.63751, std: 0.02419, params: {'max_features': 0.5, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.58121, std: 0.01928, params: {'max_features': 0.5, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.57647, std: 0.01359, params: {'max_features': 0.5, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.55525, std: 0.00663, params: {'max_features': 0.5, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.55089, std: 0.00317, params: {'max_features': 0.5, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54976, std: 0.00293, params: {'max_features': 0.5, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54868, std: 0.00331, params: {'max_features': 0.5, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54652, std: 0.00288, params: {'max_features': 0.5, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54651, std: 0.00270, params: {'max_features': 0.5, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54544, std: 0.00274, params: {'max_features': 0.5, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54463, std: 0.00246, params: {'max_features': 0.5, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54414, std: 0.00249, params: {'max_features': 0.5, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.64414, std: 0.03540, params: {'max_features': 0.75, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.58464, std: 0.01805, params: {'max_features': 0.75, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.57213, std: 0.00985, params: {'max_features': 0.75, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.55524, std: 0.00434, params: {'max_features': 0.75, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.55121, std: 0.00445, params: {'max_features': 0.75, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.55064, std: 0.00407, params: {'max_features': 0.75, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54891, std: 0.00335, params: {'max_features': 0.75, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54680, std: 0.00339, params: {'max_features': 0.75, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54625, std: 0.00357, params: {'max_features': 0.75, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54560, std: 0.00300, params: {'max_features': 0.75, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54502, std: 0.00294, params: {'max_features': 0.75, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54454, std: 0.00290, params: {'max_features': 0.75, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.64176, std: 0.03515, params: {'max_features': 1.0, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.58399, std: 0.01721, params: {'max_features': 1.0, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.57386, std: 0.01134, params: {'max_features': 1.0, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.55654, std: 0.00525, params: {'max_features': 1.0, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.55330, std: 0.00590, params: {'max_features': 1.0, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.55095, std: 0.00392, params: {'max_features': 1.0, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54840, std: 0.00342, params: {'max_features': 1.0, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54696, std: 0.00390, params: {'max_features': 1.0, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54685, std: 0.00348, params: {'max_features': 1.0, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54547, std: 0.00317, params: {'max_features': 1.0, 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54473, std: 0.00313, params: {'max_features': 1.0, 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54443, std: 0.00298, params: {'max_features': 1.0, 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.61008, std: 0.01600, params: {'max_features': 'sqrt', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.56986, std: 0.00854, params: {'max_features': 'sqrt', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.56619, std: 0.00570, params: {'max_features': 'sqrt', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.54971, std: 0.00287, params: {'max_features': 'sqrt', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54669, std: 0.00233, params: {'max_features': 'sqrt', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54682, std: 0.00243, params: {'max_features': 'sqrt', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54748, std: 0.00264, params: {'max_features': 'sqrt', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54609, std: 0.00214, params: {'max_features': 'sqrt', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54604, std: 0.00207, params: {'max_features': 'sqrt', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54776, std: 0.00228, params: {'max_features': 'sqrt', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54700, std: 0.00193, params: {'max_features': 'sqrt', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54705, std: 0.00189, params: {'max_features': 'sqrt', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.60700, std: 0.01919, params: {'max_features': 'log2', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.57235, std: 0.01103, params: {'max_features': 'log2', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.56465, std: 0.00451, params: {'max_features': 'log2', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 1}, mean: -0.54962, std: 0.00251, params: {'max_features': 'log2', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54712, std: 0.00211, params: {'max_features': 'log2', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54708, std: 0.00215, params: {'max_features': 'log2', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 5}, mean: -0.54840, std: 0.00286, params: {'max_features': 'log2', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54729, std: 0.00225, params: {'max_features': 'log2', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54684, std: 0.00210, params: {'max_features': 'log2', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 10}, mean: -0.54969, std: 0.00155, params: {'max_features': 'log2', 'n_estimators': 50, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54939, std: 0.00204, params: {'max_features': 'log2', 'n_estimators': 150, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}, mean: -0.54924, std: 0.00208, params: {'max_features': 'log2', 'n_estimators': 250, 'n_jobs': -1, 'max_depth': None, 'min_samples_leaf': 25}]\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.5, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47673\n",
      "Test set error = 0.54409\n",
      "----------\n",
      "############\n",
      "1763.1 seconds runtime\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "modelrf = RandomForestClassifier()\n",
    "\n",
    "# parameters\n",
    "estimators = [50, 150, 250] # default = 10\n",
    "features = [0.3, 0.5, 0.75, 1.0, 'sqrt', 'log2'] # default = 'sqrt'\n",
    "depths = [None] # default = None (i.e. ignored)\n",
    "samples = [1, 5, 10, 25] # default = 1\n",
    "n_jobs = [-1]\n",
    "params = {'n_estimators': estimators, 'max_features': features, 'max_depth': depths, 'min_samples_leaf': samples, \n",
    "         'n_jobs': n_jobs}\n",
    "    \n",
    "predrf = p2.get_pred(modelrf, df_all, train_y, 'RF_std_Ken_asis', track_dict=track_dict, test_idx=test_idx, train_size=0.8, \n",
    "                  columns=None, parameters=params, score_func='log_loss', n_folds=5, predict=True)\n",
    "\n",
    "print '%0.1f seconds runtime' % (time.time() - start)\n",
    "\n",
    "# write predictions\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47597\n",
      "Test set error = 0.54099\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49217\n",
      "Test set error = 0.54584\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50383\n",
      "Test set error = 0.54101\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47322\n",
      "Test set error = 0.54325\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49131\n",
      "Test set error = 0.54197\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50215\n",
      "Test set error = 0.54464\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47210\n",
      "Test set error = 0.54208\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49046\n",
      "Test set error = 0.54184\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50136\n",
      "Test set error = 0.54490\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47617\n",
      "Test set error = 0.53948\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49269\n",
      "Test set error = 0.54419\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50421\n",
      "Test set error = 0.54010\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47285\n",
      "Test set error = 0.54797\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48984\n",
      "Test set error = 0.54892\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50054\n",
      "Test set error = 0.55186\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47063\n",
      "Test set error = 0.54763\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49318\n",
      "Test set error = 0.53199\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50174\n",
      "Test set error = 0.54399\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47588\n",
      "Test set error = 0.54063\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49452\n",
      "Test set error = 0.53788\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50422\n",
      "Test set error = 0.54265\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47433\n",
      "Test set error = 0.53811\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49271\n",
      "Test set error = 0.53725\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50405\n",
      "Test set error = 0.53611\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=25, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47148\n",
      "Test set error = 0.54402\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=35, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49082\n",
      "Test set error = 0.54282\n",
      "----------\n",
      "############\n",
      "############\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "            min_samples_leaf=45, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50120\n",
      "Test set error = 0.54429\n",
      "----------\n",
      "############\n",
      "509.6 seconds runtime\n"
     ]
    }
   ],
   "source": [
    "# take out grid search options\n",
    "start = time.time()\n",
    "\n",
    "# parameters\n",
    "estimators = [250, 350, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "depths = [None] # default = None (i.e. ignored)\n",
    "samples = [25, 35, 45] # default = 1\n",
    "n_jobs = [-1]\n",
    "params = {'n_estimators': estimators, 'max_features': features, 'max_depth': depths, 'min_samples_leaf': samples, \n",
    "         'n_jobs': n_jobs}\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            modelrf = RandomForestClassifier(n_jobs=-1, n_estimators=e, max_features=f, min_samples_leaf=s)\n",
    "            get_pred(modelrf, df_all, train_y, 'RF_Ken_asis', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                     columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "\n",
    "print '%0.1f seconds runtime' % (time.time() - start)\n",
    "\n",
    "# write predictions\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
    "            min_samples_leaf=35, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "----------\n",
    "Training set error = 0.49318\n",
    "Test set error = 0.53199\n",
    "\n",
    "# 0.54119 on Kaggle \n",
    "\n",
    "# TODO\n",
    "# more trees, smaller max_features, higher min_samples_leaf\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
    "            min_samples_leaf=45, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "----------\n",
    "Training set error = 0.50405\n",
    "Test set error = 0.53611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelrf = RandomForestClassifier(n_jobs=-1, n_estimators=350, max_features=0.9, min_samples_leaf=35)\n",
    "predrf = p2.fit_and_predict(modelrf, df_all, train_y, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_file('predictions/RF_Ken_asis2.csv', predrf[:, 1], test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48887762040303029"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y, modelrf.predict_proba(df_all[:test_idx])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=250, random_state=None)\n",
      "----------\n",
      "Training set error = 0.69154\n",
      "Test set error = 0.69167\n",
      "----------\n",
      "############\n",
      "############\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=350, random_state=None)\n",
      "----------\n",
      "Training set error = 0.69200\n",
      "Test set error = 0.69203\n",
      "----------\n",
      "############\n",
      "############\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=500, random_state=None)\n",
      "----------\n",
      "Training set error = 0.69232\n",
      "Test set error = 0.69237\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "modelrfada = AdaBoostClassifier()\n",
    "\n",
    "estimators = [250, 350, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "params = {'n_estimators': estimators, 'max_features': features, 'max_depth': depths, 'min_samples_leaf': samples, \n",
    "         'n_jobs': n_jobs}\n",
    "\n",
    "for e in estimators:\n",
    "    modelrfada = AdaBoostClassifier(n_estimators=e)\n",
    "    p2.get_pred(modelrfada, df_all, train_y, 'RFAda_std', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "             columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "\n",
    "# print '%0.1f seconds runtime' % (time.time() - start)\n",
    "\n",
    "# write predictions\n",
    "#write_to_file('predictions/RFada_Ken_asis.csv', predrfada, test_ids)\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49444\n",
      "Test set error = 0.53207\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50633\n",
      "Test set error = 0.54558\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51588\n",
      "Test set error = 0.54384\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48750\n",
      "Test set error = 0.54112\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50110\n",
      "Test set error = 0.55204\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51295\n",
      "Test set error = 0.54243\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48500\n",
      "Test set error = 0.53621\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49998\n",
      "Test set error = 0.54558\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51077\n",
      "Test set error = 0.54228\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49331\n",
      "Test set error = 0.53672\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50695\n",
      "Test set error = 0.54186\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51640\n",
      "Test set error = 0.54190\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48906\n",
      "Test set error = 0.53567\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50441\n",
      "Test set error = 0.53749\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51301\n",
      "Test set error = 0.54202\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48355\n",
      "Test set error = 0.54332\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50239\n",
      "Test set error = 0.53774\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=350, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50662\n",
      "Test set error = 0.55646\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48921\n",
      "Test set error = 0.55354\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50691\n",
      "Test set error = 0.54505\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51553\n",
      "Test set error = 0.54323\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48969\n",
      "Test set error = 0.53288\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50460\n",
      "Test set error = 0.53736\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51372\n",
      "Test set error = 0.54002\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=25, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48284\n",
      "Test set error = 0.54505\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=35, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50171\n",
      "Test set error = 0.53884\n",
      "----------\n",
      "############\n",
      "############\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features=0.9, max_leaf_nodes=None,\n",
      "           min_samples_leaf=45, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51160\n",
      "Test set error = 0.53793\n",
      "----------\n",
      "############\n",
      "387.4 seconds runtime\n"
     ]
    }
   ],
   "source": [
    "# TODO use n_jobs=-1\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "estimators = [250, 350, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "depths = [None] # default = None (i.e. ignored)\n",
    "n_jobs = [-1]\n",
    "params = {'n_estimators': estimators, 'max_features': features, 'max_depth': depths, 'min_samples_leaf': samples, \n",
    "         'n_jobs': n_jobs}\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            modelrfextra = ExtraTreesClassifier(n_jobs=-1, n_estimators=e, max_features=f, min_samples_leaf=s)\n",
    "            get_pred(modelrfextra, df_all, train_y, 'RF_Ken_asis', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                     columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "print '%0.1f seconds runtime' % (time.time() - start)\n",
    "\n",
    "# write predictions\n",
    "#write_to_file('predictions/RFextra_Ken_asis.csv', predrfextra, test_ids)\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "           max_depth=None, max_features=0.75, max_leaf_nodes=None,\n",
    "           min_samples_leaf=25, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
    "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
    "----------\n",
    "Training set error = 0.48969\n",
    "Test set error = 0.53288\n",
    "\n",
    "\n",
    "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
    "           max_depth=None, max_features=0.6, max_leaf_nodes=None,\n",
    "           min_samples_leaf=25, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=-1,\n",
    "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
    "----------\n",
    "Training set error = 0.49444\n",
    "Test set error = 0.53207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Boosting/Gradient Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelboost = GradientBoostingClassifier(n_estimators=500, max_features=0.75, min_samples_leaf=35, max_depth=3)\n",
    "\n",
    "predboost = p2.fit_and_predict(modelboost, df_all, train_y, test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15995633,  0.84004367],\n",
       "       [ 0.33497891,  0.66502109],\n",
       "       [ 0.18132659,  0.81867341],\n",
       "       ..., \n",
       "       [ 0.49257866,  0.50742134],\n",
       "       [ 0.2894821 ,  0.7105179 ],\n",
       "       [ 0.51221351,  0.48778649]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50923440679552079"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y, modelboost.predict_proba(df_all[:test_idx])[:, 1])\n",
    "# all: 0.50945267198479394\n",
    " \n",
    "# Kaggle 0.53411\n",
    "# validation 0.50710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file('predictions/RFBOOST_Ken_asis.csv', predboost[:, 1], test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, predict_array, ids):\n",
    "    predictions = pd.DataFrame({'Id': ids, 'lapsed': predict_array})\n",
    "        \n",
    "    # write to csv, with header, drop index\n",
    "    predictions.to_csv(filename, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred = pd.DataFrame(modelboost.predict_proba(df_all[:test_idx])[:, 1]).rename(columns={0: 'lapsed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43336</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43337</th>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43338</th>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43339</th>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43340</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43341</th>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43342</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43343</th>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43344</th>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43345</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43346</th>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43347</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43348</th>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43349</th>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43350</th>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43351</th>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43352</th>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43353</th>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43354</th>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43355</th>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43356</th>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43357</th>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43358</th>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43359</th>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43360</th>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43361</th>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43362</th>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43363</th>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43364</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43365</th>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43366</th>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43367</th>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43368</th>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43369</th>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43370</th>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43371</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43372</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43373</th>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43374</th>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43375</th>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43376</th>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43377</th>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43378</th>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43379</th>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43380</th>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43381</th>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43382</th>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43383</th>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43384</th>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43385</th>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43386</th>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43387</th>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43388</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43389</th>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43390</th>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43391</th>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43392</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43393</th>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43394</th>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43395</th>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43396</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43397</th>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43398</th>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43399</th>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43400</th>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43401</th>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43402</th>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43403</th>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43404</th>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43410</th>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43411</th>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43412</th>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43413</th>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43414</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43415</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43416</th>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43417</th>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43418</th>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43419</th>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43420</th>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43421</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43422</th>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43423</th>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43424</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43425</th>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43426</th>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43427</th>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43428</th>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43429</th>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43430</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43431</th>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43432</th>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43433</th>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43434</th>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43435</th>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43436 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lapsed\n",
       "0        0.94\n",
       "1        0.31\n",
       "2        0.54\n",
       "3        0.50\n",
       "4        0.85\n",
       "5        0.73\n",
       "6        0.31\n",
       "7        0.76\n",
       "8        0.34\n",
       "9        0.25\n",
       "10       0.89\n",
       "11       0.21\n",
       "12       0.69\n",
       "13       0.24\n",
       "14       0.30\n",
       "15       0.64\n",
       "16       0.56\n",
       "17       0.83\n",
       "18       0.91\n",
       "19       0.58\n",
       "20       0.53\n",
       "21       0.67\n",
       "22       0.90\n",
       "23       0.37\n",
       "24       0.06\n",
       "25       0.76\n",
       "26       0.62\n",
       "27       0.21\n",
       "28       0.93\n",
       "29       0.25\n",
       "30       0.70\n",
       "31       0.93\n",
       "32       0.77\n",
       "33       0.16\n",
       "34       0.49\n",
       "35       0.68\n",
       "36       0.43\n",
       "37       0.51\n",
       "38       0.62\n",
       "39       0.73\n",
       "40       0.31\n",
       "41       0.72\n",
       "42       0.95\n",
       "43       0.16\n",
       "44       0.61\n",
       "45       0.71\n",
       "46       0.76\n",
       "47       0.90\n",
       "48       0.84\n",
       "49       0.19\n",
       "50       0.55\n",
       "51       0.71\n",
       "52       0.91\n",
       "53       0.73\n",
       "54       0.56\n",
       "55       0.80\n",
       "56       0.41\n",
       "57       0.79\n",
       "58       0.52\n",
       "59       0.76\n",
       "60       0.40\n",
       "61       0.84\n",
       "62       0.48\n",
       "63       0.36\n",
       "64       0.47\n",
       "65       0.67\n",
       "66       0.47\n",
       "67       0.02\n",
       "68       0.26\n",
       "69       0.44\n",
       "70       0.28\n",
       "71       0.50\n",
       "72       0.55\n",
       "73       0.53\n",
       "74       0.77\n",
       "75       0.45\n",
       "76       0.89\n",
       "77       0.43\n",
       "78       0.40\n",
       "79       0.54\n",
       "80       0.95\n",
       "81       0.73\n",
       "82       0.75\n",
       "83       0.53\n",
       "84       0.82\n",
       "85       0.89\n",
       "86       0.74\n",
       "87       0.18\n",
       "88       0.56\n",
       "89       0.84\n",
       "90       0.10\n",
       "91       0.35\n",
       "92       0.18\n",
       "93       0.84\n",
       "94       0.76\n",
       "95       0.40\n",
       "96       0.90\n",
       "97       0.86\n",
       "98       0.04\n",
       "99       0.40\n",
       "...       ...\n",
       "43336    0.75\n",
       "43337    0.59\n",
       "43338    0.46\n",
       "43339    0.71\n",
       "43340    0.66\n",
       "43341    0.77\n",
       "43342    0.88\n",
       "43343    0.47\n",
       "43344    0.92\n",
       "43345    0.75\n",
       "43346    0.91\n",
       "43347    0.75\n",
       "43348    0.59\n",
       "43349    0.18\n",
       "43350    0.72\n",
       "43351    0.46\n",
       "43352    0.41\n",
       "43353    0.91\n",
       "43354    0.39\n",
       "43355    0.59\n",
       "43356    0.73\n",
       "43357    0.36\n",
       "43358    0.15\n",
       "43359    0.60\n",
       "43360    0.53\n",
       "43361    0.38\n",
       "43362    0.33\n",
       "43363    0.22\n",
       "43364    0.26\n",
       "43365    0.27\n",
       "43366    0.17\n",
       "43367    0.25\n",
       "43368    0.64\n",
       "43369    0.61\n",
       "43370    0.20\n",
       "43371    0.89\n",
       "43372    0.40\n",
       "43373    0.27\n",
       "43374    0.62\n",
       "43375    0.81\n",
       "43376    0.69\n",
       "43377    0.30\n",
       "43378    0.90\n",
       "43379    0.40\n",
       "43380    0.31\n",
       "43381    0.81\n",
       "43382    0.10\n",
       "43383    0.55\n",
       "43384    0.81\n",
       "43385    0.38\n",
       "43386    0.57\n",
       "43387    0.49\n",
       "43388    0.76\n",
       "43389    0.34\n",
       "43390    0.24\n",
       "43391    0.55\n",
       "43392    0.76\n",
       "43393    0.69\n",
       "43394    0.51\n",
       "43395    0.29\n",
       "43396    0.89\n",
       "43397    0.78\n",
       "43398    0.46\n",
       "43399    0.72\n",
       "43400    0.41\n",
       "43401    0.63\n",
       "43402    0.18\n",
       "43403    0.79\n",
       "43404    0.16\n",
       "43405    0.70\n",
       "43406    0.72\n",
       "43407    0.58\n",
       "43408    0.93\n",
       "43409    0.55\n",
       "43410    0.70\n",
       "43411    0.65\n",
       "43412    0.22\n",
       "43413    0.79\n",
       "43414    0.26\n",
       "43415    0.75\n",
       "43416    0.93\n",
       "43417    0.90\n",
       "43418    0.54\n",
       "43419    0.46\n",
       "43420    0.75\n",
       "43421    0.26\n",
       "43422    0.84\n",
       "43423    0.94\n",
       "43424    0.26\n",
       "43425    0.62\n",
       "43426    0.83\n",
       "43427    0.05\n",
       "43428    0.29\n",
       "43429    0.74\n",
       "43430    0.66\n",
       "43431    0.85\n",
       "43432    0.51\n",
       "43433    0.52\n",
       "43434    0.45\n",
       "43435    0.67\n",
       "\n",
       "[43436 rows x 1 columns]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred.to_csv('predictions/train_GBRF.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'age', u'sex', u'region', u'nregions', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor', u'age.na', u'r1.na', u'r2.na', u'r3.na', u'r.quick.na', u'r.intl.na', u'mon_less30', u'mon_31', u'mon_32', u'mon_33', u'mon_34', u'mon_35', u'mon_36', u'mon_37_60', u'mon_61_84', u'mon_85_120', u'mon_121_263', u'mon_264_plus', u'games_0',\n",
       "       u'games_1_5', u'games_6_10', u'games_11_20', u'games_21_34', u'games_35_49', u'games_50_plus', u'agesq', u'agecbd', u'allgames1yrsq', u'allgames1yrcbd', u'allgames5yrsq', u'allgames5yrcbd', u'memmonthssq', u'memmonthscbd', u'memtypeA', u'memtypeF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAFtCAYAAAAQ4vChAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/RhTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1087a5050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO feature importances??\n",
    "# Females\n",
    "importance_list = modelboost.feature_importances_\n",
    "name_list = df_all.columns\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "# just get top (in reverse order)\n",
    "top_imp = importance_list[-30:]\n",
    "top_names = name_list[-30:]\n",
    "plt.barh(range(len(top_names)),top_imp,align='center')\n",
    "plt.yticks(range(len(top_names)),top_names)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Top Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53288\n",
      "Test set error = 0.54246\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51956\n",
      "Test set error = 0.53664\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47696\n",
      "Test set error = 0.54856\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53492\n",
      "Test set error = 0.53633\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51742\n",
      "Test set error = 0.54235\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48193\n",
      "Test set error = 0.53124\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53299\n",
      "Test set error = 0.54146\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51826\n",
      "Test set error = 0.54374\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48240\n",
      "Test set error = 0.53706\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53307\n",
      "Test set error = 0.54311\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51859\n",
      "Test set error = 0.53867\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47715\n",
      "Test set error = 0.53882\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53333\n",
      "Test set error = 0.54299\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51990\n",
      "Test set error = 0.53294\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48156\n",
      "Test set error = 0.53405\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53106\n",
      "Test set error = 0.54792\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51938\n",
      "Test set error = 0.53822\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48373\n",
      "Test set error = 0.53398\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53481\n",
      "Test set error = 0.53373\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51945\n",
      "Test set error = 0.53814\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47776\n",
      "Test set error = 0.53738\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53349\n",
      "Test set error = 0.54156\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51928\n",
      "Test set error = 0.53500\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47987\n",
      "Test set error = 0.53992\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53363\n",
      "Test set error = 0.53994\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52056\n",
      "Test set error = 0.53069\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48125\n",
      "Test set error = 0.53724\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53091\n",
      "Test set error = 0.53283\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51349\n",
      "Test set error = 0.53518\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46349\n",
      "Test set error = 0.54226\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52795\n",
      "Test set error = 0.54764\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51366\n",
      "Test set error = 0.53286\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46335\n",
      "Test set error = 0.54322\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52785\n",
      "Test set error = 0.54595\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51428\n",
      "Test set error = 0.53653\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46989\n",
      "Test set error = 0.53806\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52997\n",
      "Test set error = 0.53704\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51152\n",
      "Test set error = 0.53922\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46222\n",
      "Test set error = 0.53955\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.53089\n",
      "Test set error = 0.53462\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51254\n",
      "Test set error = 0.53732\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46254\n",
      "Test set error = 0.54205\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52984\n",
      "Test set error = 0.53750\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51498\n",
      "Test set error = 0.53189\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46758\n",
      "Test set error = 0.53668\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52901\n",
      "Test set error = 0.54130\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51231\n",
      "Test set error = 0.53578\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45707\n",
      "Test set error = 0.54499\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52964\n",
      "Test set error = 0.54088\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51209\n",
      "Test set error = 0.53570\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46585\n",
      "Test set error = 0.53923\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52884\n",
      "Test set error = 0.54275\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51325\n",
      "Test set error = 0.53746\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47207\n",
      "Test set error = 0.52554\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52530\n",
      "Test set error = 0.53905\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50495\n",
      "Test set error = 0.53517\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44277\n",
      "Test set error = 0.54302\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52671\n",
      "Test set error = 0.53542\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50490\n",
      "Test set error = 0.53745\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44799\n",
      "Test set error = 0.54153\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52729\n",
      "Test set error = 0.53213\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50730\n",
      "Test set error = 0.53005\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45181\n",
      "Test set error = 0.53423\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52572\n",
      "Test set error = 0.53674\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50548\n",
      "Test set error = 0.53226\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44270\n",
      "Test set error = 0.54014\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52581\n",
      "Test set error = 0.53673\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50710\n",
      "Test set error = 0.52294\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44911\n",
      "Test set error = 0.53317\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52520\n",
      "Test set error = 0.54101\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50267\n",
      "Test set error = 0.54533\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44967\n",
      "Test set error = 0.54464\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52437\n",
      "Test set error = 0.54148\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50350\n",
      "Test set error = 0.53584\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43844\n",
      "Test set error = 0.54748\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52565\n",
      "Test set error = 0.53711\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50518\n",
      "Test set error = 0.53363\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44595\n",
      "Test set error = 0.53896\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52571\n",
      "Test set error = 0.53447\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50356\n",
      "Test set error = 0.53721\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45143\n",
      "Test set error = 0.53491\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# parameters\n",
    "estimators = [250, 350, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "max_depth = [2, 3, 5]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                modelboost = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d)\n",
    "                get_pred(modelboost, df_all, train_y, 'RFBoost_Ken_asis', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                         columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=35, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.50710\n",
    "Test set error = 0.52294\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
    "              min_samples_leaf=45, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.52056\n",
    "Test set error = 0.53069\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=35, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.48193\n",
    "Test set error = 0.53124\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=45, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.51498\n",
    "Test set error = 0.53189\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=350,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.53091\n",
    "Test set error = 0.53283\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=35, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.51990\n",
    "Test set error = 0.53294\n",
    "\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.53481\n",
    "Test set error = 0.53373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52540\n",
      "Test set error = 0.53787\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50429\n",
      "Test set error = 0.54128\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44336\n",
      "Test set error = 0.53655\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52417\n",
      "Test set error = 0.54237\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50462\n",
      "Test set error = 0.54156\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44885\n",
      "Test set error = 0.54692\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52376\n",
      "Test set error = 0.54514\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50561\n",
      "Test set error = 0.53854\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45440\n",
      "Test set error = 0.54209\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52518\n",
      "Test set error = 0.53824\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50066\n",
      "Test set error = 0.54891\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44115\n",
      "Test set error = 0.54020\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52348\n",
      "Test set error = 0.54507\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50356\n",
      "Test set error = 0.53868\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44923\n",
      "Test set error = 0.53826\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52667\n",
      "Test set error = 0.53394\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50511\n",
      "Test set error = 0.53503\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45120\n",
      "Test set error = 0.54543\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52579\n",
      "Test set error = 0.53434\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50437\n",
      "Test set error = 0.53690\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43982\n",
      "Test set error = 0.54724\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52461\n",
      "Test set error = 0.54017\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50300\n",
      "Test set error = 0.53880\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44693\n",
      "Test set error = 0.54139\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52501\n",
      "Test set error = 0.54023\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50698\n",
      "Test set error = 0.52826\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45283\n",
      "Test set error = 0.53882\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51956\n",
      "Test set error = 0.53969\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49250\n",
      "Test set error = 0.53825\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41319\n",
      "Test set error = 0.55065\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52016\n",
      "Test set error = 0.53922\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49169\n",
      "Test set error = 0.54146\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42650\n",
      "Test set error = 0.53868\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51984\n",
      "Test set error = 0.54040\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49259\n",
      "Test set error = 0.54696\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43489\n",
      "Test set error = 0.53929\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51976\n",
      "Test set error = 0.53808\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49321\n",
      "Test set error = 0.53289\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41346\n",
      "Test set error = 0.54721\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51828\n",
      "Test set error = 0.54451\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49431\n",
      "Test set error = 0.53169\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42153\n",
      "Test set error = 0.55395\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51848\n",
      "Test set error = 0.54592\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49458\n",
      "Test set error = 0.53727\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43143\n",
      "Test set error = 0.54309\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51904\n",
      "Test set error = 0.53973\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49051\n",
      "Test set error = 0.53789\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41054\n",
      "Test set error = 0.54351\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51779\n",
      "Test set error = 0.54390\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49067\n",
      "Test set error = 0.54039\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42685\n",
      "Test set error = 0.54254\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51924\n",
      "Test set error = 0.53929\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49468\n",
      "Test set error = 0.53682\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42880\n",
      "Test set error = 0.55045\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51570\n",
      "Test set error = 0.53403\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48362\n",
      "Test set error = 0.52883\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.38584\n",
      "Test set error = 0.56413\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51259\n",
      "Test set error = 0.54840\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48284\n",
      "Test set error = 0.54218\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.40300\n",
      "Test set error = 0.55220\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51560\n",
      "Test set error = 0.53813\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48367\n",
      "Test set error = 0.54412\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.40946\n",
      "Test set error = 0.54755\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51554\n",
      "Test set error = 0.53406\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48141\n",
      "Test set error = 0.53952\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.39121\n",
      "Test set error = 0.54375\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51321\n",
      "Test set error = 0.54715\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48284\n",
      "Test set error = 0.54067\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.39838\n",
      "Test set error = 0.55845\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51693\n",
      "Test set error = 0.53029\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48501\n",
      "Test set error = 0.53654\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41362\n",
      "Test set error = 0.54365\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51217\n",
      "Test set error = 0.54634\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48070\n",
      "Test set error = 0.53881\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.38380\n",
      "Test set error = 0.55429\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51657\n",
      "Test set error = 0.53008\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48321\n",
      "Test set error = 0.53503\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=40, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.39883\n",
      "Test set error = 0.54845\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=2, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51420\n",
      "Test set error = 0.54045\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48362\n",
      "Test set error = 0.54354\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=55, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.40789\n",
      "Test set error = 0.55548\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "# parameters\n",
    "estimators = [500, 750, 1000] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 40, 55] # default = 1\n",
    "max_depth = [2, 3, 5]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                modelboost = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d)\n",
    "                get_pred(modelboost, df_all, train_y, 'RFBoost_Ken_asis', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                         columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
    "              min_samples_leaf=55, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.50698\n",
    "Test set error = 0.52826\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.48362\n",
    "Test set error = 0.52883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Classifier - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GaussianNB()\n",
      "----------\n",
      "Training set error = 3.62745\n",
      "Test set error = 3.55768\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "clfGNB = GaussianNB()\n",
    "get_pred(clfGNB, df_all, train_y, 'NaiveBayes', track_dict=None, test_idx=test_idx, train_size=0.8, columns=None,\n",
    "                  parameters=None, score_func='log_loss', predict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "----------\n",
      "Training set error = 0.56045\n",
      "Test set error = 0.56567\n",
      "----------\n",
      "############\n",
      "588.7 seconds runtime\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "modelsvc = SVC(probability=True, C=0.5)\n",
    "\n",
    "Cs=[0.01] # default = 1, try 0.01, 0.1, 1, 10, 100\n",
    "# penalties = ['l1', 'l2'] \n",
    "params = {'C': Cs, 'probability': [True]} # 'penalty': penalties\n",
    "\n",
    "predlr = p2.get_pred(modelsvc, df_all, train_y, 'SVM', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                     columns=None, parameters=None, score_func='log_loss', n_folds=5, predict=False)\n",
    "\n",
    "print '%0.1f seconds runtime' % (time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "----------\n",
    "Training set error = 0.55647\n",
    "Test set error = 0.57190\n",
    "----------\n",
    "############\n",
    "572.6 seconds runtime\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "----------\n",
    "Training set error = 0.56045\n",
    "Test set error = 0.56567\n",
    "----------\n",
    "############\n",
    "588.7 seconds runtime\n",
    "\n",
    "\n",
    "############\n",
    "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "----------\n",
    "Training set error = 0.56553\n",
    "Test set error = 0.56934\n",
    "----------\n",
    "############\n",
    "595.3 seconds runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/dev/modules/neural_networks_supervised.htmlsklearn \n",
    "# http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(34748, 2)]\n",
      "[(8688, 2)]\n",
      "############\n",
      "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
      "      f_stable=0.001,\n",
      "      hidden0=<sknn.nn.Layer `Softmax`: units=2, name=u'hidden0', frozen=False>,\n",
      "      layers=[<sknn.nn.Layer `Softmax`: units=2, name=u'hidden0', frozen=False>, <sknn.nn.Layer `Softmax`: units=2, name=u'hidden0', frozen=False>],\n",
      "      learning_momentum=0.9, learning_rate=0.001, learning_rule=u'sgd',\n",
      "      loss_type=None, n_iter=100, n_stable=10, normalize=None,\n",
      "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
      "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)\n",
      "----------\n",
      "Training set error = 0.55307\n",
      "Test set error = 0.54706\n",
      "----------\n",
      "############\n",
      "295.7 seconds runtime\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Softmax\")]*2,\n",
    "    learning_rate=0.001,\n",
    "    n_iter=100)\n",
    "\n",
    "p2.get_pred(nn, df_all, train_y, 'NN', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                     columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "\n",
    "print '%0.1f seconds runtime' % (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try more layers 2 layers: 100 iterations, 0.001 learning rate\n",
    "\n",
    "# 100 iterations, 0.001 learning rate: \n",
    "[(34748, 2)]\n",
    "[(8688, 2)]\n",
    "############\n",
    "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
    "      f_stable=0.001,\n",
    "      layers=[<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
    "      learning_momentum=0.9, learning_rate=0.001, learning_rule=u'sgd',\n",
    "      loss_type=None, n_iter=100, n_stable=10, normalize=None,\n",
    "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
    "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
    "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)\n",
    "----------\n",
    "Training set error = 0.55157\n",
    "Test set error = 0.54468\n",
    "----------\n",
    "############\n",
    "262.3 seconds runtime\n",
    "\n",
    "\n",
    "# 25 iterations all below: \n",
    "# 0.001 learning rate\n",
    "[(34748, 2)]\n",
    "[(8688, 2)]\n",
    "############\n",
    "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
    "      f_stable=0.001,\n",
    "      layers=[<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
    "      learning_momentum=0.9, learning_rate=0.001, learning_rule=u'sgd',\n",
    "      loss_type=None, n_iter=25, n_stable=10, normalize=None,\n",
    "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
    "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
    "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)\n",
    "----------\n",
    "Training set error = 0.55012\n",
    "Test set error = 0.55049\n",
    "----------\n",
    "############\n",
    "\n",
    "\n",
    "# .0001 learning rate\n",
    "[(34748, 2)]\n",
    "[(8688, 2)]\n",
    "############\n",
    "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
    "      f_stable=0.001,\n",
    "      layers=[<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
    "      learning_momentum=0.9, learning_rate=0.0001, learning_rule=u'sgd',\n",
    "      loss_type=None, n_iter=25, n_stable=10, normalize=None,\n",
    "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
    "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
    "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)\n",
    "----------\n",
    "Training set error = 0.55203\n",
    "Test set error = 0.55114\n",
    "----------\n",
    "############\n",
    "\n",
    "# 0.01 learning rate\n",
    "\n",
    "[(34748, 2)]\n",
    "[(8688, 2)]\n",
    "############\n",
    "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
    "      f_stable=0.001,\n",
    "      layers=[<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
    "      learning_momentum=0.9, learning_rate=0.01, learning_rule=u'sgd',\n",
    "      loss_type=None, n_iter=25, n_stable=10, normalize=None,\n",
    "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
    "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
    "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)\n",
    "----------\n",
    "Training set error = 0.60972\n",
    "Test set error = 0.61454\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.42591\n",
      "Test set error = 2.02984\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.42294\n",
      "Test set error = 2.09477\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.42318\n",
      "Test set error = 2.10851\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00155\n",
      "Test set error = 2.15902\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00166\n",
      "Test set error = 2.10344\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00125\n",
      "Test set error = 2.18960\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=25, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.52473\n",
      "Test set error = 0.61177\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=25, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.52425\n",
      "Test set error = 0.60707\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=25, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.52411\n",
      "Test set error = 0.60571\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=25, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00151\n",
      "Test set error = 0.66704\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=25, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00196\n",
      "Test set error = 0.62236\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=25, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00166\n",
      "Test set error = 0.66524\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.54847\n",
      "Test set error = 0.55903\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.54734\n",
      "Test set error = 0.56260\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.54847\n",
      "Test set error = 0.56075\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00151\n",
      "Test set error = 0.62294\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00155\n",
      "Test set error = 0.61522\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='distance')\n",
      "----------\n",
      "Training set error = 0.00159\n",
      "Test set error = 0.60058\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "n_neighbors = [5, 25, 100] # default = 5\n",
    "weights = ['uniform', 'distance']\n",
    "leaf_size = [10, 30, 100]\n",
    "\n",
    "for n in n_neighbors:\n",
    "    for w in weights: \n",
    "        for l in leaf_size: \n",
    "            knn = KNeighborsClassifier(n_neighbors=n, weights=w, leaf_size=l, n_jobs=-1)\n",
    "            p2.get_pred(knn, df_all, train_y, 'KNN', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                     columns=None, parameters=None, score_func='log_loss', predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
    "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
    "           weights='uniform')\n",
    "----------\n",
    "Training set error = 0.54847\n",
    "Test set error = 0.55903\n",
    "----------\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.54838\n",
      "Test set error = 0.55940\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.54757\n",
      "Test set error = 0.56459\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.54774\n",
      "Test set error = 0.56683\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=200, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.55848\n",
      "Test set error = 0.55248\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=200, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.55641\n",
      "Test set error = 0.56047\n",
      "----------\n",
      "############\n",
      "############\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=200, p=2,\n",
      "           weights='uniform')\n",
      "----------\n",
      "Training set error = 0.55374\n",
      "Test set error = 0.56795\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "n_neighbors = [100, 200] # default = 5\n",
    "leaf_size = [10, 30, 100]\n",
    "\n",
    "for n in n_neighbors: \n",
    "    for l in leaf_size: \n",
    "        knn = KNeighborsClassifier(n_neighbors=n, leaf_size=l, n_jobs=-1)\n",
    "        p2.get_pred(knn, df_all, train_y, 'KNN', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "                 columns=None, parameters=None, score_func='log_loss', predict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-324af59816bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m p2.get_pred(eclf, df_all, train_y, 'voting_std', track_dict=None, test_idx=test_idx, train_size=0.8, \n\u001b[0;32m---> 20\u001b[0;31m              columns=None, parameters=None, score_func='log_loss', predict=False)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/Documents/Classes/Spring-2016/Stat149/stat149project/Amy/amyutility.pyc\u001b[0m in \u001b[0;36mget_pred\u001b[0;34m(model, dataframe, train_y, model_name, track_dict, test_idx, train_size, columns, parameters, score_func, n_folds, predict)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# get train and test set error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/ensemble/voting_classifier.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mfitted_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitted_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 290\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "clf1 = LogisticRegression(penalty='l1', C=1)\n",
    "clf2 = RandomForestClassifier(n_jobs=-1, n_estimators=350, max_features=0.9, min_samples_leaf=35)\n",
    "clf3 = GradientBoostingClassifier(n_estimators=500, max_features=0.75, min_samples_leaf=35, max_depth=3)\n",
    "clf4 = nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.001,\n",
    "    n_iter=100)\n",
    "clf5 = SVC(probability=True, C=0.5)\n",
    "clf6 = KNeighborsClassifier(n_neighbors=200, leaf_size=10, n_jobs=-1)\n",
    "\n",
    "# TODO experiment with weights  weights=[2,1,1]\n",
    "# predict_proba is not available when voting='hard'\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), \n",
    "                                   ('nn', clf4), ('svc', clf5), ('knn', clf6)], voting='soft')\n",
    "\n",
    "p2.get_pred(eclf, df_all, train_y, 'voting_std', track_dict=None, test_idx=test_idx, train_size=0.8, \n",
    "             columns=None, parameters=None, score_func='log_loss', predict=False)\n",
    "\n",
    "\n",
    "print '%0.1f seconds runtime' % (time.time() - start)\n",
    "\n",
    "\n",
    "# Test set error: 0.55104\n",
    "# Kaggle: 0.54159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14479, 2)]\n"
     ]
    }
   ],
   "source": [
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gb', clf3), \n",
    "                                   ('nn', clf4), ('svc', clf5), ('knn', clf6)], voting='soft')\n",
    "\n",
    "pred = p2.fit_and_predict(eclf, df_all, train_y, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14479,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p2.write_to_file('predictions/majority_std.csv', pred, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No weights\n",
    "############\n",
    "VotingClassifier(estimators=[('lr', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)), ('rf', RandomFor...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False))],\n",
    "         voting='soft', weights=None)\n",
    "----------\n",
    "Training set error = 0.50992\n",
    "Test set error = 0.53194\n",
    "----------\n",
    "############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
