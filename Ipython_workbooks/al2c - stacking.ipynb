{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# true, then pred\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import amyutility as p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'amyutility' from 'amyutility.pyc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43436, 23)\n",
      "(14479, 23)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traink = pd.read_csv('data/fromKen/full_train_2.csv')\n",
    "testk = pd.read_csv('data/fromKen/full_test_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43436, 56)\n",
      "(14479, 55)\n"
     ]
    }
   ],
   "source": [
    "print traink.shape\n",
    "print testk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg127</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>2024.00</td>\n",
       "      <td>nan</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg142</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>258</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2753.00</td>\n",
       "      <td>2751.00</td>\n",
       "      <td>2709.00</td>\n",
       "      <td>3528.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>10</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg104</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1668.00</td>\n",
       "      <td>2910.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>M</td>\n",
       "      <td>reg112</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>14</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>741.00</td>\n",
       "      <td>1107.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.00</td>\n",
       "      <td>F</td>\n",
       "      <td>reg106</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>131</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>359.00</td>\n",
       "      <td>325.00</td>\n",
       "      <td>531.00</td>\n",
       "      <td>654.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>nan</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  Id\n",
       "0 29.00   M  reg127         1       N          2        Y        N        N     nan     nan 2024.00      nan     N    N     nan            0            0           0          0           0       0   1\n",
       "1 16.00   M  reg142         1       N        258        N        N        Y 2753.00 2751.00 2709.00  3528.00     N    N     nan           10          223           0         57           7       0   2\n",
       "2 22.00   M  reg104         1       N         28        N        N        Y     nan     nan 1668.00  2910.00     N    N     nan            6            6           2          1           0       0   3\n",
       "3 10.00   M  reg112         1       N         14        N        N        Y     nan     nan  741.00  1107.00     N    N     nan           13           13           0          2           1       0   4\n",
       "4 14.00   F  reg106         1       N        131        N        N        N  359.00  325.00  531.00   654.00     N    N     nan           14           57           0         16           1       0   5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'lapsed', u'age', u'sex', u'region', u'nregions', u'memtype', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ken's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'lapsed', u'age', u'sex', u'region', u'nregions', u'memtype', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor', u'age.na', u'r1.na', u'r2.na', u'r3.na', u'r.quick.na', u'r.intl.na', u'mon_less30', u'mon_31', u'mon_32', u'mon_33', u'mon_34', u'mon_35', u'mon_36', u'mon_37_60', u'mon_61_84', u'mon_85_120', u'mon_121_263', u'mon_264_plus',\n",
       "       u'games_0', u'games_1_5', u'games_6_10', u'games_11_20', u'games_21_34', u'games_35_49', u'games_50_plus', u'agesq', u'agecbd', u'allgames1yrsq', u'allgames1yrcbd', u'allgames5yrsq', u'allgames5yrcbd', u'memmonthssq', u'memmonthscbd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'age', u'sex', u'region', u'nregions', u'memtype', u'memmonths', u'mem_mag1', u'mem_mag2', u'hasemail', u'r1', u'r2', u'r3', u'r.quick', u'extra', u'intl', u'r.intl', u'allgames1yr', u'allgames5yr', u'fastevents', u'medevents', u'slowevents', u'nfloor', u'age.na', u'r1.na', u'r2.na', u'r3.na', u'r.quick.na', u'r.intl.na', u'mon_less30', u'mon_31', u'mon_32', u'mon_33', u'mon_34', u'mon_35', u'mon_36', u'mon_37_60', u'mon_61_84', u'mon_85_120', u'mon_121_263', u'mon_264_plus',\n",
       "       u'games_0', u'games_1_5', u'games_6_10', u'games_11_20', u'games_21_34', u'games_35_49', u'games_50_plus', u'agesq', u'agecbd', u'allgames1yrsq', u'allgames1yrcbd', u'allgames5yrsq', u'allgames5yrcbd', u'memmonthssq', u'memmonthscbd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lapsed</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>11.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "      <td>61.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>16.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>47.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>11.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lapsed   age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  \\\n",
       "0      Y 11.00   M    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00   \n",
       "1      N 61.00   M    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83   \n",
       "2      Y 16.00   F    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20   \n",
       "3      Y 47.00   M    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00   \n",
       "4      Y 11.00   F    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69   \n",
       "\n",
       "   allgames5yrsq  allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0           0.00            0.00         5.99          8.99  \n",
       "1           6.80           10.20        10.59         15.88  \n",
       "2           6.80           10.20        10.53         15.79  \n",
       "3           0.00            0.00        11.19         16.78  \n",
       "4           7.17           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>2024.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>258</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2753.00</td>\n",
       "      <td>2751.00</td>\n",
       "      <td>2709.00</td>\n",
       "      <td>3528.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>10</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>4.80</td>\n",
       "      <td>7.19</td>\n",
       "      <td>10.82</td>\n",
       "      <td>16.23</td>\n",
       "      <td>11.11</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1668.00</td>\n",
       "      <td>2910.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.27</td>\n",
       "      <td>9.41</td>\n",
       "      <td>3.89</td>\n",
       "      <td>5.84</td>\n",
       "      <td>3.89</td>\n",
       "      <td>5.84</td>\n",
       "      <td>6.73</td>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>14</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>741.00</td>\n",
       "      <td>1107.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.80</td>\n",
       "      <td>7.19</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.92</td>\n",
       "      <td>5.28</td>\n",
       "      <td>7.92</td>\n",
       "      <td>5.42</td>\n",
       "      <td>8.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>131</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>359.00</td>\n",
       "      <td>325.00</td>\n",
       "      <td>531.00</td>\n",
       "      <td>654.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.42</td>\n",
       "      <td>8.12</td>\n",
       "      <td>5.42</td>\n",
       "      <td>8.12</td>\n",
       "      <td>8.12</td>\n",
       "      <td>12.18</td>\n",
       "      <td>9.77</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 29.00   M    0.11         1       N          2        Y        N        N 1942.12 1811.61 2024.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      0           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   6.80   10.20           0.00            0.00           0.00   \n",
       "1 16.00   M    0.02         1       N        258        N        N        Y 2753.00 2751.00 2709.00  3528.00     N    N 3477.56           10          223           0         57           7       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False       True       False       False       False          True   5.67    8.50           4.80            7.19          10.82   \n",
       "2 22.00   M    0.00         1       N         28        N        N        Y 1942.12 1811.61 1668.00  2910.00     N    N 3477.56            6            6           2          1           0       0       0      1      1      0           0          1       True  False  False  False  False  False  False     False     False      False       False        False   False     False       True       False       False       False         False   6.27    9.41           3.89            5.84           3.89   \n",
       "3 10.00   M    0.12         1       N         14        N        N        Y 1942.12 1811.61  741.00  1107.00     N    N 3477.56           13           13           0          2           1       0       0      1      1      0           0          1       True  False  False  False  False  False  False     False     False      False       False        False   False     False      False        True       False       False         False   4.80    7.19           5.28            7.92           5.28   \n",
       "4 14.00   F    0.04         1       N        131        N        N        N  359.00  325.00  531.00   654.00     N    N 3477.56           14           57           0         16           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False        True       False       False          True   5.42    8.12           5.42            8.12           8.12   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0            0.00         2.20          3.30  \n",
       "1           16.23        11.11         16.67  \n",
       "2            5.84         6.73         10.10  \n",
       "3            7.92         5.42          8.12  \n",
       "4           12.18         9.77         14.65  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traink_y = traink[['lapsed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lapsed\n",
       "0      Y\n",
       "1      N\n",
       "2      Y\n",
       "3      Y\n",
       "4      Y"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traink_x = traink.drop('lapsed', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43436, 55)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00   M    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00   M    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00   F    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00   M    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00   F    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0            0.00         5.99          8.99  \n",
       "1           10.20        10.59         15.88  \n",
       "2           10.20        10.53         15.79  \n",
       "3            0.00        11.19         16.78  \n",
       "4           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traink_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = (traink_y.lapsed.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key train_y\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 14477, 14478, 14479])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key test_ids: for writing to predictions\n",
    "test_ids = test.Id.values\n",
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# key df_all: combine test and train into df_all, test_idx \n",
    "test_idx = traink_x.shape[0]\n",
    "df_all = pd.concat((traink_x, testk), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57915, 55)\n",
      "43436\n"
     ]
    }
   ],
   "source": [
    "print df_all.shape\n",
    "print test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key: \n",
    "- df_all\n",
    "- test_idx\n",
    "- train_y\n",
    "- test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Types and Convert\n",
    "\n",
    "- need to convert sex, memtype, mem_mag1, mem_mag2, hasemail, extra, intl\n",
    "- Can leave bools alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONVERT = ['sex', 'memtype', 'mem_mag1', 'mem_mag2', 'hasemail', 'extra', 'intl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sex\n",
    "\n",
    "- males 0\n",
    "- females 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFeCAYAAACvnuTEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKABAcf4PyfuyvtiPZrgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d1f29d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mostly male (males 0, females 1)\n",
    "gender = df_all.groupby('sex').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "gender.plot(kind='barh', title = 'Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>M</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>F</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00   M    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00   M    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00   F    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00   M    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00   F    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0            0.00         5.99          8.99  \n",
       "1           10.20        10.59         15.88  \n",
       "2           10.20        10.53         15.79  \n",
       "3            0.00        11.19         16.78  \n",
       "4           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               float64\n",
       "sex                object\n",
       "region            float64\n",
       "nregions            int64\n",
       "memtype            object\n",
       "memmonths           int64\n",
       "mem_mag1           object\n",
       "mem_mag2           object\n",
       "hasemail           object\n",
       "r1                float64\n",
       "r2                float64\n",
       "r3                float64\n",
       "r.quick           float64\n",
       "extra              object\n",
       "intl               object\n",
       "r.intl            float64\n",
       "allgames1yr         int64\n",
       "allgames5yr         int64\n",
       "fastevents          int64\n",
       "medevents           int64\n",
       "slowevents          int64\n",
       "nfloor              int64\n",
       "age.na              int64\n",
       "r1.na               int64\n",
       "r2.na               int64\n",
       "r3.na               int64\n",
       "r.quick.na          int64\n",
       "r.intl.na           int64\n",
       "mon_less30           bool\n",
       "mon_31               bool\n",
       "mon_32               bool\n",
       "mon_33               bool\n",
       "mon_34               bool\n",
       "mon_35               bool\n",
       "mon_36               bool\n",
       "mon_37_60            bool\n",
       "mon_61_84            bool\n",
       "mon_85_120           bool\n",
       "mon_121_263          bool\n",
       "mon_264_plus         bool\n",
       "games_0              bool\n",
       "games_1_5            bool\n",
       "games_6_10           bool\n",
       "games_11_20          bool\n",
       "games_21_34          bool\n",
       "games_35_49          bool\n",
       "games_50_plus        bool\n",
       "agesq             float64\n",
       "agecbd            float64\n",
       "allgames1yrsq     float64\n",
       "allgames1yrcbd    float64\n",
       "allgames5yrsq     float64\n",
       "allgames5yrcbd    float64\n",
       "memmonthssq       float64\n",
       "memmonthscbd      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 M\n",
       "1                 M\n",
       "2                 F\n",
       "3                 M\n",
       "4                 F\n",
       "5                 F\n",
       "6                 M\n",
       "7                 M\n",
       "8                 M\n",
       "9                 M\n",
       "10                M\n",
       "11                M\n",
       "12                M\n",
       "13                M\n",
       "14                M\n",
       "15                F\n",
       "16                M\n",
       "17                M\n",
       "18                F\n",
       "19                M\n",
       "20                M\n",
       "21                F\n",
       "22                M\n",
       "23                M\n",
       "24                M\n",
       "25                M\n",
       "26                M\n",
       "27                M\n",
       "28                F\n",
       "29                M\n",
       "30                M\n",
       "31                M\n",
       "32                M\n",
       "33                F\n",
       "34                M\n",
       "35                M\n",
       "36                M\n",
       "37                M\n",
       "38                M\n",
       "39                F\n",
       "40                M\n",
       "41                M\n",
       "42                M\n",
       "43                F\n",
       "44                M\n",
       "45                M\n",
       "46                M\n",
       "47                M\n",
       "48                M\n",
       "49                M\n",
       "50                M\n",
       "51                M\n",
       "52                F\n",
       "53                M\n",
       "54                M\n",
       "55                F\n",
       "56                M\n",
       "57                M\n",
       "58                M\n",
       "59                M\n",
       "60                M\n",
       "61       Z_dummy_NA\n",
       "62                M\n",
       "63                M\n",
       "64                M\n",
       "65                M\n",
       "66                M\n",
       "67                M\n",
       "68                M\n",
       "69                M\n",
       "70                M\n",
       "71                M\n",
       "72                M\n",
       "73                M\n",
       "74                M\n",
       "75                M\n",
       "76                M\n",
       "77                M\n",
       "78                M\n",
       "79                M\n",
       "80       Z_dummy_NA\n",
       "81                M\n",
       "82                F\n",
       "83                M\n",
       "84                M\n",
       "85       Z_dummy_NA\n",
       "86                F\n",
       "87                M\n",
       "88                M\n",
       "89                M\n",
       "90                M\n",
       "91                M\n",
       "92                M\n",
       "93                M\n",
       "94                M\n",
       "95                M\n",
       "96                M\n",
       "97                M\n",
       "98                M\n",
       "99                M\n",
       "            ...    \n",
       "14379             M\n",
       "14380             M\n",
       "14381             M\n",
       "14382             M\n",
       "14383             M\n",
       "14384             F\n",
       "14385             M\n",
       "14386             M\n",
       "14387             M\n",
       "14388             M\n",
       "14389             M\n",
       "14390             M\n",
       "14391             M\n",
       "14392             M\n",
       "14393             M\n",
       "14394             M\n",
       "14395             M\n",
       "14396             M\n",
       "14397             M\n",
       "14398             M\n",
       "14399             M\n",
       "14400             M\n",
       "14401             M\n",
       "14402             M\n",
       "14403             M\n",
       "14404             M\n",
       "14405             M\n",
       "14406             F\n",
       "14407             M\n",
       "14408             M\n",
       "14409             F\n",
       "14410             M\n",
       "14411             M\n",
       "14412             M\n",
       "14413             M\n",
       "14414             M\n",
       "14415             M\n",
       "14416             M\n",
       "14417             M\n",
       "14418             M\n",
       "14419             M\n",
       "14420             M\n",
       "14421             F\n",
       "14422             M\n",
       "14423             M\n",
       "14424    Z_dummy_NA\n",
       "14425             M\n",
       "14426             F\n",
       "14427             M\n",
       "14428             M\n",
       "14429             M\n",
       "14430             M\n",
       "14431             F\n",
       "14432             M\n",
       "14433             M\n",
       "14434             M\n",
       "14435             M\n",
       "14436             M\n",
       "14437             M\n",
       "14438             M\n",
       "14439    Z_dummy_NA\n",
       "14440             M\n",
       "14441             M\n",
       "14442             M\n",
       "14443             M\n",
       "14444             M\n",
       "14445             M\n",
       "14446             M\n",
       "14447             M\n",
       "14448             M\n",
       "14449             F\n",
       "14450             M\n",
       "14451             M\n",
       "14452             M\n",
       "14453             M\n",
       "14454             M\n",
       "14455             M\n",
       "14456             M\n",
       "14457             F\n",
       "14458             M\n",
       "14459             M\n",
       "14460             M\n",
       "14461             M\n",
       "14462             M\n",
       "14463             M\n",
       "14464             M\n",
       "14465             M\n",
       "14466             F\n",
       "14467             M\n",
       "14468             M\n",
       "14469             F\n",
       "14470             M\n",
       "14471             M\n",
       "14472             M\n",
       "14473             F\n",
       "14474    Z_dummy_NA\n",
       "14475             F\n",
       "14476             M\n",
       "14477             M\n",
       "14478             M\n",
       "Name: sex, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.sex = (df_all.sex.values=='F')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memtype</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>198</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>192</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>268</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>101</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions memtype  memmonths mem_mag1 mem_mag2 hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  \\\n",
       "0 11.00    0    0.12         1       N         19        N        N        N 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00   \n",
       "1 61.00    0    0.12         1       N        198        Y        N        Y 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83   \n",
       "2 16.00    1    0.12         1       N        192        N        N        Y  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20   \n",
       "3 47.00    0    0.12         1       N        268        Y        N        Y 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00   \n",
       "4 11.00    1    0.12         1       N        101        N        N        N  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69   \n",
       "\n",
       "   allgames5yrsq  allgames5yrcbd  memmonthssq  memmonthscbd  \n",
       "0           0.00            0.00         5.99          8.99  \n",
       "1           6.80           10.20        10.59         15.88  \n",
       "2           6.80           10.20        10.53         15.79  \n",
       "3           0.00            0.00        11.19         16.78  \n",
       "4           7.17           10.75         9.25         13.87  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memtype\n",
    "- make Normal the reference category\n",
    "- memtypeA=1 for affiliate\n",
    "- memtypeF=1 for family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d89bf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# membership types A, F, N \n",
    "memtype = df_all.groupby('memtype').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "memtype.plot(kind='barh', title = 'Memtype')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['memtypeA'] = (df_all.memtype=='A')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['memtypeF'] = (df_all.memtype=='F')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = df_all.drop('memtype', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mem_mag1 & mem_mag2 & hasemail\n",
    "\n",
    "- only yes or no... convert yes to 1, no to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGAM8QYAwBjiDQCAMcQbAABj/g8VjRY/U6OGdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103b08290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memmag1 = df_all.groupby('mem_mag1').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "memmag1.plot(kind='barh', title = 'mem_mag1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFeCAYAAAB+T51FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d1f2110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memmag2 = df_all.groupby('mem_mag2').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "memmag2.plot(kind='barh', title = 'mem_mag2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcQbAABjiDcAAMYQbwAAjCHeAAAY828pa7jssE1jJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d348bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasemail = df_all.groupby('hasemail').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "hasemail.plot(kind='barh', title = 'hasemail')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.mem_mag1 = (df_all.mem_mag1.values=='Y')*1\n",
    "df_all.mem_mag2 = (df_all.mem_mag2.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.hasemail = (df_all.hasemail.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail      r1      r2      r3  r.quick extra intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00    0    0.12         1         19         0         0         0 1942.12 1811.61 1557.56  2007.74     N    N 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00    0    0.12         1        198         1         0         1 2178.00 2215.00 2291.00  2932.00     Y    N 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00    1    0.12         1        192         0         0         1  627.00  628.00 1362.00  2007.00     N    N 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00    0    0.12         1        268         1         0         1 2600.00 2601.00 2602.00  2007.74     N    N 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00    1    0.12         1        101         0         0         0  464.00  466.00  958.00  1356.00     N    N 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  \n",
       "0            0.00         5.99          8.99         0         0  \n",
       "1           10.20        10.59         15.88         0         0  \n",
       "2           10.20        10.53         15.79         0         0  \n",
       "3            0.00        11.19         16.78         0         0  \n",
       "4           10.75         9.25         13.87         0         0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra, intl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFeCAYAAAB+T51FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e889ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extra = df_all.groupby('extra').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "extra.plot(kind='barh', title = 'extra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFeCAYAAACsH5cdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3gBQjHgDQDHiDQDFiDcAFPM/+ic1i4HfanwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd82750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intl = df_all.groupby('intl').size().sort_values(ascending = True)/df_all.shape[0]*100\n",
    "intl.plot(kind='barh', title = 'intl')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.extra = (df_all.extra.values=='Y')*1\n",
    "df_all.intl = (df_all.intl.values=='Y')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail      r1      r2      r3  r.quick  extra  intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20 games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  \\\n",
       "0 11.00    0    0.12         1         19         0         0         0 1942.12 1811.61 1557.56  2007.74      0     0 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False       False       False         False   4.97    7.45           0.00            0.00           0.00   \n",
       "1 61.00    0    0.12         1        198         1         0         1 2178.00 2215.00 2291.00  2932.00      1     0 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False       False       False          True   8.25   12.38           3.22            4.83           6.80   \n",
       "2 16.00    1    0.12         1        192         0         0         1  627.00  628.00 1362.00  2007.00      0     0 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False        True       False          True   5.67    8.50           6.80           10.20           6.80   \n",
       "3 47.00    0    0.12         1        268         1         0         1 2600.00 2601.00 2602.00  2007.74      0     0 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False       False       False          True   7.74   11.61           0.00            0.00           0.00   \n",
       "4 11.00    1    0.12         1        101         0         0         0  464.00  466.00  958.00  1356.00      0     0 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True       False       False          True   4.97    7.45           5.13            7.69           7.17   \n",
       "\n",
       "   allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  \n",
       "0            0.00         5.99          8.99         0         0  \n",
       "1           10.20        10.59         15.88         0         0  \n",
       "2           10.20        10.53         15.79         0         0  \n",
       "3            0.00        11.19         16.78         0         0  \n",
       "4           10.75         9.25         13.87         0         0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### % change in chess rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['r3r2'] = df_all.r3 / df_all.r2 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['r3r1'] = df_all.r3 / df_all.r1 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['allgames_change'] = df_all.allgames1yr/((df_all.allgames5yr - df_all.allgames1yr + 1)/4)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['fastevets_prop'] = df_all.fastevents / (df_all.fastevents + df_all.medevents + df_all.slowevents+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['medevents_prop'] = df_all.medevents / (df_all.fastevents + df_all.medevents + df_all.slowevents+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['slowevents_prop'] = df_all.slowevents / (df_all.fastevents + df_all.medevents + df_all.slowevents+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO region is discrete?? \n",
    "# last 8 are sq and cubed terms \n",
    "STANDARDIZE = ['age', 'region', 'nregions', 'memmonths', 'r1', 'r2', 'r3', 'r.quick', 'r.intl', \n",
    "               'allgames1yr', 'allgames5yr', 'fastevents', 'medevents', 'slowevents', 'nfloor', \n",
    "              'agesq', 'agecbd', 'allgames1yrsq', 'allgames1yrcbd', 'allgames5yrsq', 'allgames5yrcbd', 'memmonthssq', \n",
    "               'memmonthscbd', 'r3r2', 'r3r1', 'allgames_change', 'fastevets_prop', 'medevents_prop', 'slowevents_prop']\n",
    "\n",
    "\n",
    "INDICATORS = ['sex', 'mem_mag1', 'mem_mag2', 'hasemail', 'extra', 'intl', 'age.na', 'r1.na', 'r2.na', \n",
    "             'r3.na', 'r.quick.na', 'r.intl.na', 'mon_less30', 'mon_31', 'mon_32', 'mon_33', 'mon_34', \n",
    "             'mon_35', 'mon_36', 'mon_37_60', 'mon_61_84', 'mon_85_120', 'mon_121_263', 'mon_264_plus', \n",
    "             'games_0', 'games_1_5', 'games_6_10', 'games_11_20', 'games_21_34', 'games_35_49', 'games_50_plus', \n",
    "             'memtypeA', 'memtypeF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STANDARDIZE) + len(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms_train_x = pd.read_csv('data/fromKen/stacking/trainX.csv')\n",
    "# ms_train_y = pd.read_csv('data/fromKen/stacking/trainY.csv')\n",
    "ms_test_x = pd.read_csv('data/fromKen/stacking/testX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43436, 9)\n",
      "(43436, 1)\n",
      "(14479, 9)\n"
     ]
    }
   ],
   "source": [
    "print ms_train_x.shape\n",
    "print ms_train_y.shape\n",
    "print ms_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glmpreds</th>\n",
       "      <th>gampreds</th>\n",
       "      <th>rf1preds</th>\n",
       "      <th>rf2preds</th>\n",
       "      <th>nn1preds</th>\n",
       "      <th>nn2preds</th>\n",
       "      <th>gbpolypreds</th>\n",
       "      <th>svmpreds</th>\n",
       "      <th>knnpreds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   glmpreds  gampreds  rf1preds  rf2preds  nn1preds  nn2preds  gbpolypreds  svmpreds  knnpreds\n",
       "0      0.94      0.94      1.00      1.00      0.95      0.95         0.94      0.78      0.92\n",
       "1      0.44      0.42      0.46      0.44      0.41      0.37         0.19      0.34      0.48\n",
       "2      0.54      0.55      0.65      0.66      0.63      0.57         0.77      0.71      0.72\n",
       "3      0.54      0.54      0.39      0.36      0.47      0.40         0.48      0.41      0.40\n",
       "4      0.89      0.88      0.83      0.83      0.83      0.85         0.82      0.82      0.68"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'glmpreds', u'gampreds', u'rf1preds', u'rf2preds', u'nn1preds', u'nn2preds', u'gbpolypreds', u'svmpreds', u'knnpreds'], dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_train_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2   V3   V4   V5   V6   V7   V8   V9\n",
       "0 0.82 0.81 0.81 0.81 0.86 0.87 0.84 0.78 0.74\n",
       "1 0.59 0.61 0.65 0.64 0.78 0.79 0.67 0.65 0.52\n",
       "2 0.85 0.85 0.77 0.79 0.85 0.85 0.82 0.78 0.64\n",
       "3 0.60 0.61 0.40 0.45 0.59 0.57 0.57 0.70 0.50\n",
       "4 0.86 0.87 0.83 0.83 0.88 0.88 0.90 0.84 0.82"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'V1', u'V2', u'V3', u'V4', u'V5', u'V6', u'V7', u'V8', u'V9'], dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_test_x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendrick Lo [11:44 PM] \n",
    "hey… i think i’d like to pick some features that are commonly in the top for the random forest as opposed to just sticking them all back in… otherwise i think the other stuff will just drown out our stacked predictions\n",
    "\n",
    "[11:48] \n",
    "how about this then… maybe i will give you the file with the predictions..  you can run that on its own through GBM and then do a second run with those and all yours in the RF\n",
    "\n",
    "[11:48] \n",
    "then we will cover both basis\n",
    "\n",
    "[11:48] \n",
    "i’ll use top 5 from my models in my RF\n",
    "\n",
    "\n",
    "\n",
    "I think the column in the responses has to be renamed to “lapsed\"\n",
    "\n",
    "[1:56] \n",
    "Also, the test set’s columns names need to be changed to the same column names as the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ms_test_x.rename(columns={'V1': 'glmpreds', 'V2': 'gampreds', 'V3': 'rf1preds', 'V4': 'rf2preds', 'V5': 'nn1preds', \n",
    "                         'V6': 'nn2preds', 'V7': 'gbpolypreds', 'V8': 'svmpreds', 'V9': 'knnpreds'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glmpreds</th>\n",
       "      <th>gampreds</th>\n",
       "      <th>rf1preds</th>\n",
       "      <th>rf2preds</th>\n",
       "      <th>nn1preds</th>\n",
       "      <th>nn2preds</th>\n",
       "      <th>gbpolypreds</th>\n",
       "      <th>svmpreds</th>\n",
       "      <th>knnpreds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   glmpreds  gampreds  rf1preds  rf2preds  nn1preds  nn2preds  gbpolypreds  svmpreds  knnpreds\n",
       "0      0.82      0.81      0.81      0.81      0.86      0.87         0.84      0.78      0.74\n",
       "1      0.59      0.61      0.65      0.64      0.78      0.79         0.67      0.65      0.52\n",
       "2      0.85      0.85      0.77      0.79      0.85      0.85         0.82      0.78      0.64\n",
       "3      0.60      0.61      0.40      0.45      0.59      0.57         0.57      0.70      0.50\n",
       "4      0.86      0.87      0.83      0.83      0.88      0.88         0.90      0.84      0.82"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_stacking = pd.concat([ms_train_x, ms_test_x], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57915, 9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_stacking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57915, 62)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all_stacking, df_all], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57915, 71)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Females\n",
    "importance_list = clfForest.feature_importances_\n",
    "name_list = all_features\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "# just get top (in reverse order)\n",
    "top_imp = importance_list[-20:]\n",
    "top_names = name_list[-20:]\n",
    "plt.barh(range(len(top_names)),top_imp,align='center')\n",
    "plt.yticks(range(len(top_names)),top_names)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Top 20 Features for Females')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- df_all\n",
    "- df_all_stacking\n",
    "- test_idx\n",
    "- train_y\n",
    "- test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51194\n",
      "Test set error = 0.53270\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46368\n",
      "Test set error = 0.53794\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.27812\n",
      "Test set error = 0.55228\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51277\n",
      "Test set error = 0.53623\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46991\n",
      "Test set error = 0.53791\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.29816\n",
      "Test set error = 0.55520\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51096\n",
      "Test set error = 0.54386\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47233\n",
      "Test set error = 0.53519\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.32014\n",
      "Test set error = 0.55891\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51253\n",
      "Test set error = 0.53197\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46493\n",
      "Test set error = 0.54328\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.28963\n",
      "Test set error = 0.54995\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50852\n",
      "Test set error = 0.55043\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46725\n",
      "Test set error = 0.54060\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.30819\n",
      "Test set error = 0.55102\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51219\n",
      "Test set error = 0.53273\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47288\n",
      "Test set error = 0.52702\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.31166\n",
      "Test set error = 0.55512\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51255\n",
      "Test set error = 0.53081\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46214\n",
      "Test set error = 0.53419\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.27353\n",
      "Test set error = 0.56010\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50769\n",
      "Test set error = 0.54744\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46994\n",
      "Test set error = 0.53467\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.30971\n",
      "Test set error = 0.56094\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51006\n",
      "Test set error = 0.53847\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47021\n",
      "Test set error = 0.54034\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.31597\n",
      "Test set error = 0.56338\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49198\n",
      "Test set error = 0.53876\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41132\n",
      "Test set error = 0.54265\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.17119\n",
      "Test set error = 0.56424\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49645\n",
      "Test set error = 0.52902\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41623\n",
      "Test set error = 0.54617\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.19841\n",
      "Test set error = 0.57808\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49506\n",
      "Test set error = 0.53705\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42643\n",
      "Test set error = 0.53943\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.21758\n",
      "Test set error = 0.57532\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49462\n",
      "Test set error = 0.53076\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41217\n",
      "Test set error = 0.54194\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.16603\n",
      "Test set error = 0.57978\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49407\n",
      "Test set error = 0.53663\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41893\n",
      "Test set error = 0.54270\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.20161\n",
      "Test set error = 0.57672\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49459\n",
      "Test set error = 0.53302\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42145\n",
      "Test set error = 0.54590\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.20527\n",
      "Test set error = 0.57105\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49247\n",
      "Test set error = 0.53704\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41186\n",
      "Test set error = 0.54638\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.17760\n",
      "Test set error = 0.57524\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49151\n",
      "Test set error = 0.53990\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41793\n",
      "Test set error = 0.54126\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.19599\n",
      "Test set error = 0.56752\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49101\n",
      "Test set error = 0.54770\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41683\n",
      "Test set error = 0.55812\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.22321\n",
      "Test set error = 0.56259\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46102\n",
      "Test set error = 0.54201\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.33402\n",
      "Test set error = 0.55937\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.05974\n",
      "Test set error = 0.64707\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46276\n",
      "Test set error = 0.54444\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.34304\n",
      "Test set error = 0.54866\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.08883\n",
      "Test set error = 0.62778\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46543\n",
      "Test set error = 0.53346\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.34808\n",
      "Test set error = 0.55854\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=10, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.10229\n",
      "Test set error = 0.60587\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45893\n",
      "Test set error = 0.54749\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.33067\n",
      "Test set error = 0.56450\n",
      "----------\n",
      "############\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-ef22dad3b341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 p2.get_pred_np(modelboost, df_all, train_y, 'RFBoost', track_dict=None, \n\u001b[1;32m     14\u001b[0m                             \u001b[0mtest_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                             score_func='log_loss', predict=False)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/Documents/Classes/Spring-2016/Stat149/stat149project/Amy/amyutility.pyc\u001b[0m in \u001b[0;36mget_pred_np\u001b[0;34m(model, dataframe, train_y, model_name, track_dict, test_idx, train_size, columns, parameters, score_func, n_folds, predict)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_optimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# get train and test set error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1025\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1078\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1079\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 784\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amylee/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "estimators = [250, 500, 1000] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "max_depth = [3, 5, 10]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                modelboost = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d, random_state=1004)\n",
    "                p2.get_pred_np(modelboost, df_all, train_y, 'RFBoost', track_dict=None, \n",
    "                            test_idx=test_idx, train_size=0.8, columns=None, parameters=None, \n",
    "                            score_func='log_loss', predict=False)\n",
    "\n",
    "\n",
    "# option to save fitted model\n",
    "# joblib.dump(model, 'models/baseline_logistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=45, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.47288\n",
    "Test set error = 0.52702\n",
    "----------\n",
    "############\n",
    "############\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=35, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.49645\n",
    "Test set error = 0.52902\n",
    "----------\n",
    "############\n",
    "############\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.49462\n",
    "Test set error = 0.53076\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.51255\n",
    "Test set error = 0.53081\n",
    "----------\n",
    "############\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_model = GradientBoostingClassifier(n_estimators=500, max_features=0.6, min_samples_leaf=35, max_depth=3, \n",
    "                                        random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57915, 71)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_full = p2.fit_and_predict(full_model, df_all, train_y, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14479,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50070699982853328"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y, full_model.predict_proba(df_all[:test_idx])[:, 1])\n",
    "# validation 0.52902\n",
    "# Kaggle 0.53114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p2.write_to_file('predictions/stacking_gbx_all_features.csv', pred_full, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAFtCAYAAADh6assAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bdf8310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance_list = full_model.feature_importances_\n",
    "name_list = df_all.columns\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "# just get top (in reverse order)\n",
    "top_imp = importance_list[-40:]\n",
    "top_names = name_list[-40:]\n",
    "plt.barh(range(len(top_names)),top_imp,align='center')\n",
    "plt.yticks(range(len(top_names)),top_names)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Top Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nfloor',\n",
       " 'mon_35',\n",
       " 'memtypeF',\n",
       " 'memtypeA',\n",
       " 'fastevents',\n",
       " 'allgames1yr',\n",
       " 'allgames1yrsq',\n",
       " 'allgames5yrcbd',\n",
       " 'allgames5yrsq',\n",
       " 'allgames1yrcbd',\n",
       " 'allgames5yr',\n",
       " 'age',\n",
       " 'slowevents',\n",
       " 'agecbd',\n",
       " 'agesq',\n",
       " 'medevents',\n",
       " 'r.intl',\n",
       " 'fastevets_prop',\n",
       " 'memmonths',\n",
       " 'memmonthssq',\n",
       " 'slowevents_prop',\n",
       " 'medevents_prop',\n",
       " 'memmonthscbd',\n",
       " 'r3',\n",
       " 'region',\n",
       " 'knnpreds',\n",
       " 'r2',\n",
       " 'r1',\n",
       " 'gampreds',\n",
       " 'r3r1',\n",
       " 'allgames_change',\n",
       " 'rf2preds',\n",
       " 'r.quick',\n",
       " 'r3r2',\n",
       " 'glmpreds',\n",
       " 'nn2preds',\n",
       " 'rf1preds',\n",
       " 'nn1preds',\n",
       " 'svmpreds',\n",
       " 'gbpolypreds')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52441\n",
      "Test set error = 0.53666\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51678\n",
      "Test set error = 0.54090\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50855\n",
      "Test set error = 0.53237\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52641\n",
      "Test set error = 0.52914\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51621\n",
      "Test set error = 0.54323\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51032\n",
      "Test set error = 0.53012\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52450\n",
      "Test set error = 0.53717\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51986\n",
      "Test set error = 0.53122\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50875\n",
      "Test set error = 0.53818\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52319\n",
      "Test set error = 0.54083\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51640\n",
      "Test set error = 0.53810\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50683\n",
      "Test set error = 0.53850\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52456\n",
      "Test set error = 0.53454\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51600\n",
      "Test set error = 0.54034\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50935\n",
      "Test set error = 0.53177\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52448\n",
      "Test set error = 0.53616\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51952\n",
      "Test set error = 0.53208\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50877\n",
      "Test set error = 0.53713\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52329\n",
      "Test set error = 0.53883\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51715\n",
      "Test set error = 0.53445\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50586\n",
      "Test set error = 0.54012\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52435\n",
      "Test set error = 0.53474\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51824\n",
      "Test set error = 0.53179\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50639\n",
      "Test set error = 0.54218\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52535\n",
      "Test set error = 0.52987\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51673\n",
      "Test set error = 0.53987\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50879\n",
      "Test set error = 0.53569\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51658\n",
      "Test set error = 0.52679\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49949\n",
      "Test set error = 0.53443\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47842\n",
      "Test set error = 0.53535\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51448\n",
      "Test set error = 0.53580\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49808\n",
      "Test set error = 0.54253\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48022\n",
      "Test set error = 0.54003\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51447\n",
      "Test set error = 0.53870\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50288\n",
      "Test set error = 0.53039\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48387\n",
      "Test set error = 0.53569\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51649\n",
      "Test set error = 0.52631\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49779\n",
      "Test set error = 0.53906\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47771\n",
      "Test set error = 0.54020\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51460\n",
      "Test set error = 0.53600\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50167\n",
      "Test set error = 0.53040\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47928\n",
      "Test set error = 0.53801\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51368\n",
      "Test set error = 0.53893\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50013\n",
      "Test set error = 0.53913\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48361\n",
      "Test set error = 0.53733\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51256\n",
      "Test set error = 0.53700\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49696\n",
      "Test set error = 0.53362\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47536\n",
      "Test set error = 0.54339\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51431\n",
      "Test set error = 0.53142\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49707\n",
      "Test set error = 0.53949\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47826\n",
      "Test set error = 0.54695\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51378\n",
      "Test set error = 0.53570\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49919\n",
      "Test set error = 0.53869\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48275\n",
      "Test set error = 0.53776\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49990\n",
      "Test set error = 0.53394\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46768\n",
      "Test set error = 0.54788\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43521\n",
      "Test set error = 0.55297\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49925\n",
      "Test set error = 0.53787\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47294\n",
      "Test set error = 0.54407\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44442\n",
      "Test set error = 0.54684\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50130\n",
      "Test set error = 0.53389\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47551\n",
      "Test set error = 0.53840\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44812\n",
      "Test set error = 0.54406\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49798\n",
      "Test set error = 0.53896\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47340\n",
      "Test set error = 0.53903\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43465\n",
      "Test set error = 0.55127\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49978\n",
      "Test set error = 0.53421\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46902\n",
      "Test set error = 0.55005\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44417\n",
      "Test set error = 0.54348\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49864\n",
      "Test set error = 0.54419\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47629\n",
      "Test set error = 0.53295\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44798\n",
      "Test set error = 0.54762\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49806\n",
      "Test set error = 0.53571\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46873\n",
      "Test set error = 0.54611\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43471\n",
      "Test set error = 0.54556\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49956\n",
      "Test set error = 0.53388\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47118\n",
      "Test set error = 0.53776\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44043\n",
      "Test set error = 0.54905\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49825\n",
      "Test set error = 0.53822\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47312\n",
      "Test set error = 0.53963\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.44762\n",
      "Test set error = 0.54025\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "estimators = [100, 250, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                modelboost = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d, random_state=1004)\n",
    "                p2.get_pred_np(modelboost, df_all_stacking, train_y, 'RFBoost', track_dict=None, \n",
    "                            test_idx=test_idx, train_size=0.8, columns=None, parameters=None, \n",
    "                            score_func='log_loss', predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.52050\n",
    "Test set error = 0.52144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stacked2 = GradientBoostingClassifier(n_estimators=100, max_features=0.6, min_samples_leaf=25, max_depth=4, \n",
    "                                        random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_stacked = p2.fit_and_predict(model_stacked2, df_all_stacking, train_y, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14479,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52011249123484482"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y, model_stacked2.predict_proba(df_all_stacking[:test_idx])[:, 1])\n",
    "# validation 0.52144\n",
    "# Kaggle 0.53081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57915, 9)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_stacking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p2.write_to_file('predictions/stacking_gbx_no_features.csv', pred_stacked, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
       "              min_samples_leaf=25, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stacked2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbx_2nd_order_predict = p2.half_half(model_stacked2, df_all_stacking, train_y,  test_idx=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43436,)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbx_2nd_order_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53725531639596846"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y, gbx_2nd_order_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({'lapsed': gbx_2nd_order_predict})\n",
    "\n",
    "# write to csv, with header, drop index\n",
    "predictions.to_csv('predictions/train_predictions/gbx_2nd_order.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO try with just Ken's features and not my features added in from before (that's when I had best GBX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_subset = df_all.drop('r3r2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_subset = df_all_subset.drop('r3r1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_subset = df_all_subset.drop('allgames_change', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_subset = df_all_subset.drop('fastevets_prop', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_subset = df_all_subset.drop('medevents_prop', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_subset = df_all_subset.drop('slowevents_prop', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57915, 65)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glmpreds</th>\n",
       "      <th>gampreds</th>\n",
       "      <th>rf1preds</th>\n",
       "      <th>rf2preds</th>\n",
       "      <th>nn1preds</th>\n",
       "      <th>nn2preds</th>\n",
       "      <th>gbpolypreds</th>\n",
       "      <th>svmpreds</th>\n",
       "      <th>knnpreds</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.92</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.48</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   glmpreds  gampreds  rf1preds  rf2preds  nn1preds  nn2preds  gbpolypreds  svmpreds  knnpreds   age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail      r1      r2      r3  r.quick  extra  intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20  \\\n",
       "0      0.94      0.94      1.00      1.00      0.95      0.95         0.94      0.78      0.92 11.00    0    0.12         1         19         0         0         0 1942.12 1811.61 1557.56  2007.74      0     0 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False   \n",
       "1      0.44      0.42      0.46      0.44      0.41      0.37         0.19      0.34      0.48 61.00    0    0.12         1        198         1         0         1 2178.00 2215.00 2291.00  2932.00      1     0 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False   \n",
       "2      0.54      0.55      0.65      0.66      0.63      0.57         0.77      0.71      0.72 16.00    1    0.12         1        192         0         0         1  627.00  628.00 1362.00  2007.00      0     0 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False   \n",
       "3      0.54      0.54      0.39      0.36      0.47      0.40         0.48      0.41      0.40 47.00    0    0.12         1        268         1         0         1 2600.00 2601.00 2602.00  2007.74      0     0 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False   \n",
       "4      0.89      0.88      0.83      0.83      0.83      0.85         0.82      0.82      0.68 11.00    1    0.12         1        101         0         0         0  464.00  466.00  958.00  1356.00      0     0 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True   \n",
       "\n",
       "  games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  \n",
       "0       False       False         False   4.97    7.45           0.00            0.00           0.00            0.00         5.99          8.99         0         0  \n",
       "1       False       False          True   8.25   12.38           3.22            4.83           6.80           10.20        10.59         15.88         0         0  \n",
       "2        True       False          True   5.67    8.50           6.80           10.20           6.80           10.20        10.53         15.79         0         0  \n",
       "3       False       False          True   7.74   11.61           0.00            0.00           0.00            0.00        11.19         16.78         0         0  \n",
       "4       False       False          True   4.97    7.45           5.13            7.69           7.17           10.75         9.25         13.87         0         0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52455\n",
      "Test set error = 0.53356\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51644\n",
      "Test set error = 0.53191\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50024\n",
      "Test set error = 0.53727\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52550\n",
      "Test set error = 0.53009\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51583\n",
      "Test set error = 0.54118\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50282\n",
      "Test set error = 0.54117\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52385\n",
      "Test set error = 0.53509\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51652\n",
      "Test set error = 0.53449\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50511\n",
      "Test set error = 0.53253\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52247\n",
      "Test set error = 0.54155\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51364\n",
      "Test set error = 0.54341\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50221\n",
      "Test set error = 0.53156\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52474\n",
      "Test set error = 0.53162\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51468\n",
      "Test set error = 0.53562\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50404\n",
      "Test set error = 0.53594\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52351\n",
      "Test set error = 0.53662\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51358\n",
      "Test set error = 0.54335\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50700\n",
      "Test set error = 0.52677\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52338\n",
      "Test set error = 0.53517\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51571\n",
      "Test set error = 0.53263\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50108\n",
      "Test set error = 0.53779\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52546\n",
      "Test set error = 0.53097\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51681\n",
      "Test set error = 0.52993\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49969\n",
      "Test set error = 0.54639\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52401\n",
      "Test set error = 0.53449\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51449\n",
      "Test set error = 0.53661\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50584\n",
      "Test set error = 0.53385\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51044\n",
      "Test set error = 0.54163\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49360\n",
      "Test set error = 0.54034\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46893\n",
      "Test set error = 0.53884\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51253\n",
      "Test set error = 0.53784\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49453\n",
      "Test set error = 0.53894\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47129\n",
      "Test set error = 0.53763\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51393\n",
      "Test set error = 0.52784\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49577\n",
      "Test set error = 0.52995\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47491\n",
      "Test set error = 0.52931\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51334\n",
      "Test set error = 0.53011\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49271\n",
      "Test set error = 0.53723\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46927\n",
      "Test set error = 0.53679\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51270\n",
      "Test set error = 0.53161\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49552\n",
      "Test set error = 0.52912\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46882\n",
      "Test set error = 0.54606\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51125\n",
      "Test set error = 0.53831\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49564\n",
      "Test set error = 0.54038\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47403\n",
      "Test set error = 0.53008\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51203\n",
      "Test set error = 0.53832\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49629\n",
      "Test set error = 0.53075\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46425\n",
      "Test set error = 0.54072\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51084\n",
      "Test set error = 0.53514\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49479\n",
      "Test set error = 0.53174\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46980\n",
      "Test set error = 0.53592\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50906\n",
      "Test set error = 0.54389\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49546\n",
      "Test set error = 0.53206\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46846\n",
      "Test set error = 0.54659\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49427\n",
      "Test set error = 0.53812\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45975\n",
      "Test set error = 0.53938\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41783\n",
      "Test set error = 0.55165\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49387\n",
      "Test set error = 0.54364\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46140\n",
      "Test set error = 0.54237\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41982\n",
      "Test set error = 0.54476\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49539\n",
      "Test set error = 0.53498\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46446\n",
      "Test set error = 0.53865\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43017\n",
      "Test set error = 0.54415\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49208\n",
      "Test set error = 0.54623\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45810\n",
      "Test set error = 0.54495\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41658\n",
      "Test set error = 0.53806\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49415\n",
      "Test set error = 0.53717\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45883\n",
      "Test set error = 0.54397\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41728\n",
      "Test set error = 0.54979\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49461\n",
      "Test set error = 0.53925\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46308\n",
      "Test set error = 0.54380\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42695\n",
      "Test set error = 0.53423\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49384\n",
      "Test set error = 0.53511\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45478\n",
      "Test set error = 0.54844\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41067\n",
      "Test set error = 0.54558\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49595\n",
      "Test set error = 0.52976\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46201\n",
      "Test set error = 0.53826\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42060\n",
      "Test set error = 0.54151\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49336\n",
      "Test set error = 0.53963\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46205\n",
      "Test set error = 0.54374\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42377\n",
      "Test set error = 0.54603\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "estimators = [100, 250, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                model_subset = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d, random_state=1004)\n",
    "                p2.get_pred_np(model_subset, df_all_subset, train_y, 'RFBoost', track_dict=None, \n",
    "                            test_idx=test_idx, train_size=0.8, columns=None, parameters=None, \n",
    "                            score_func='log_loss', predict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=45, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.51393\n",
    "Test set error = 0.52784\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=45, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.50700\n",
    "Test set error = 0.52677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO try one with just the most important features instead of all features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nfloor',\n",
       " 'mon_35',\n",
       " 'memtypeF',\n",
       " 'memtypeA',\n",
       " 'fastevents',\n",
       " 'allgames1yr',\n",
       " 'allgames1yrsq',\n",
       " 'allgames5yrcbd',\n",
       " 'allgames5yrsq',\n",
       " 'allgames1yrcbd',\n",
       " 'allgames5yr',\n",
       " 'age',\n",
       " 'slowevents',\n",
       " 'agecbd',\n",
       " 'agesq',\n",
       " 'medevents',\n",
       " 'r.intl',\n",
       " 'fastevets_prop',\n",
       " 'memmonths',\n",
       " 'memmonthssq',\n",
       " 'slowevents_prop',\n",
       " 'medevents_prop',\n",
       " 'memmonthscbd',\n",
       " 'r3',\n",
       " 'region',\n",
       " 'knnpreds',\n",
       " 'r2',\n",
       " 'r1',\n",
       " 'gampreds',\n",
       " 'r3r1',\n",
       " 'allgames_change',\n",
       " 'rf2preds',\n",
       " 'r.quick',\n",
       " 'r3r2',\n",
       " 'glmpreds',\n",
       " 'nn2preds',\n",
       " 'rf1preds',\n",
       " 'nn1preds',\n",
       " 'svmpreds',\n",
       " 'gbpolypreds')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glmpreds</th>\n",
       "      <th>gampreds</th>\n",
       "      <th>rf1preds</th>\n",
       "      <th>rf2preds</th>\n",
       "      <th>nn1preds</th>\n",
       "      <th>nn2preds</th>\n",
       "      <th>gbpolypreds</th>\n",
       "      <th>svmpreds</th>\n",
       "      <th>knnpreds</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>nregions</th>\n",
       "      <th>memmonths</th>\n",
       "      <th>mem_mag1</th>\n",
       "      <th>mem_mag2</th>\n",
       "      <th>hasemail</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r.quick</th>\n",
       "      <th>extra</th>\n",
       "      <th>intl</th>\n",
       "      <th>r.intl</th>\n",
       "      <th>allgames1yr</th>\n",
       "      <th>allgames5yr</th>\n",
       "      <th>fastevents</th>\n",
       "      <th>medevents</th>\n",
       "      <th>slowevents</th>\n",
       "      <th>nfloor</th>\n",
       "      <th>age.na</th>\n",
       "      <th>r1.na</th>\n",
       "      <th>r2.na</th>\n",
       "      <th>r3.na</th>\n",
       "      <th>r.quick.na</th>\n",
       "      <th>r.intl.na</th>\n",
       "      <th>mon_less30</th>\n",
       "      <th>mon_31</th>\n",
       "      <th>mon_32</th>\n",
       "      <th>mon_33</th>\n",
       "      <th>mon_34</th>\n",
       "      <th>mon_35</th>\n",
       "      <th>mon_36</th>\n",
       "      <th>mon_37_60</th>\n",
       "      <th>mon_61_84</th>\n",
       "      <th>mon_85_120</th>\n",
       "      <th>mon_121_263</th>\n",
       "      <th>mon_264_plus</th>\n",
       "      <th>games_0</th>\n",
       "      <th>games_1_5</th>\n",
       "      <th>games_6_10</th>\n",
       "      <th>games_11_20</th>\n",
       "      <th>games_21_34</th>\n",
       "      <th>games_35_49</th>\n",
       "      <th>games_50_plus</th>\n",
       "      <th>agesq</th>\n",
       "      <th>agecbd</th>\n",
       "      <th>allgames1yrsq</th>\n",
       "      <th>allgames1yrcbd</th>\n",
       "      <th>allgames5yrsq</th>\n",
       "      <th>allgames5yrcbd</th>\n",
       "      <th>memmonthssq</th>\n",
       "      <th>memmonthscbd</th>\n",
       "      <th>memtypeA</th>\n",
       "      <th>memtypeF</th>\n",
       "      <th>r3r2</th>\n",
       "      <th>r3r1</th>\n",
       "      <th>allgames_change</th>\n",
       "      <th>fastevets_prop</th>\n",
       "      <th>medevents_prop</th>\n",
       "      <th>slowevents_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.92</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1942.12</td>\n",
       "      <td>1811.61</td>\n",
       "      <td>1557.56</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.48</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2178.00</td>\n",
       "      <td>2215.00</td>\n",
       "      <td>2291.00</td>\n",
       "      <td>2932.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.83</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.59</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>627.00</td>\n",
       "      <td>628.00</td>\n",
       "      <td>1362.00</td>\n",
       "      <td>2007.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>10.53</td>\n",
       "      <td>15.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.17</td>\n",
       "      <td>115.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>47.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2600.00</td>\n",
       "      <td>2601.00</td>\n",
       "      <td>2602.00</td>\n",
       "      <td>2007.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>7.74</td>\n",
       "      <td>11.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.19</td>\n",
       "      <td>16.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.68</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>466.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>1356.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3477.56</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.97</td>\n",
       "      <td>7.45</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.17</td>\n",
       "      <td>10.75</td>\n",
       "      <td>9.25</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   glmpreds  gampreds  rf1preds  rf2preds  nn1preds  nn2preds  gbpolypreds  svmpreds  knnpreds   age  sex  region  nregions  memmonths  mem_mag1  mem_mag2  hasemail      r1      r2      r3  r.quick  extra  intl  r.intl  allgames1yr  allgames5yr  fastevents  medevents  slowevents  nfloor  age.na  r1.na  r2.na  r3.na  r.quick.na  r.intl.na mon_less30 mon_31 mon_32 mon_33 mon_34 mon_35 mon_36 mon_37_60 mon_61_84 mon_85_120 mon_121_263 mon_264_plus games_0 games_1_5 games_6_10 games_11_20  \\\n",
       "0      0.94      0.94      1.00      1.00      0.95      0.95         0.94      0.78      0.92 11.00    0    0.12         1         19         0         0         0 1942.12 1811.61 1557.56  2007.74      0     0 3477.56            0            0           0          0           0       0       0      1      1      1           1          1       True  False  False  False  False  False  False     False     False      False       False        False    True     False      False       False   \n",
       "1      0.44      0.42      0.46      0.44      0.41      0.37         0.19      0.34      0.48 61.00    0    0.12         1        198         1         0         1 2178.00 2215.00 2291.00  2932.00      1     0 3477.56            4           29           1          0          10       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False      True      False       False   \n",
       "2      0.54      0.55      0.65      0.66      0.63      0.57         0.77      0.71      0.72 16.00    1    0.12         1        192         0         0         1  627.00  628.00 1362.00  2007.00      0     0 3477.56           29           29           0          4           1       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False      False        True        False   False     False      False       False   \n",
       "3      0.54      0.54      0.39      0.36      0.47      0.40         0.48      0.41      0.40 47.00    0    0.12         1        268         1         0         1 2600.00 2601.00 2602.00  2007.74      0     0 3477.56            0            0           0          0           0       0       0      0      0      0           1          1      False  False  False  False  False  False  False     False     False      False       False         True    True     False      False       False   \n",
       "4      0.89      0.88      0.83      0.83      0.83      0.85         0.82      0.82      0.68 11.00    1    0.12         1        101         0         0         0  464.00  466.00  958.00  1356.00      0     0 3477.56           12           35           0          8           0       0       0      0      0      0           0          1      False  False  False  False  False  False  False     False     False       True       False        False   False     False      False        True   \n",
       "\n",
       "  games_21_34 games_35_49 games_50_plus  agesq  agecbd  allgames1yrsq  allgames1yrcbd  allgames5yrsq  allgames5yrcbd  memmonthssq  memmonthscbd  memtypeA  memtypeF  r3r2  r3r1  allgames_change  fastevets_prop  medevents_prop  slowevents_prop  \n",
       "0       False       False         False   4.97    7.45           0.00            0.00           0.00            0.00         5.99          8.99         0         0 -0.14 -0.20            -1.00            0.00            0.00             0.00  \n",
       "1       False       False          True   8.25   12.38           3.22            4.83           6.80           10.20        10.59         15.88         0         0  0.03  0.05            -0.38            0.09            0.00             0.91  \n",
       "2        True       False          True   5.67    8.50           6.80           10.20           6.80           10.20        10.53         15.79         0         0  1.17  1.17           115.00            0.00            0.80             0.20  \n",
       "3       False       False          True   7.74   11.61           0.00            0.00           0.00            0.00        11.19         16.78         0         0  0.00  0.00            -1.00            0.00            0.00             0.00  \n",
       "4       False       False          True   4.97    7.45           5.13            7.69           7.17           10.75         9.25         13.87         0         0  1.06  1.06             1.00            0.00            1.00             0.00  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glmpreds</th>\n",
       "      <th>gampreds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14380</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14381</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14382</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14383</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14384</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14385</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14386</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14388</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14389</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14390</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14391</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14392</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14393</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14394</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14395</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14396</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14397</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14399</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14400</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14401</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14402</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14403</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14404</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14405</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14406</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14409</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14410</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14411</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14412</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14413</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14414</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14415</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14417</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14418</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14419</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14420</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14421</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14422</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14423</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14424</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14425</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14426</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14427</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14429</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14430</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14431</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14432</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14433</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14434</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14435</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14436</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14437</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14438</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14439</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14440</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14441</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14446</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14447</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14448</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14449</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14450</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14451</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14452</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14453</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14454</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14455</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14456</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14457</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14458</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14459</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14460</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14461</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14462</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14463</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14464</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14465</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14466</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14467</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14468</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14469</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14470</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14471</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14474</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14475</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14476</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14477</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14478</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57915 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       glmpreds  gampreds\n",
       "0          0.94      0.94\n",
       "1          0.44      0.42\n",
       "2          0.54      0.55\n",
       "3          0.54      0.54\n",
       "4          0.89      0.88\n",
       "5          0.79      0.78\n",
       "6          0.33      0.34\n",
       "7          0.81      0.77\n",
       "8          0.43      0.43\n",
       "9          0.25      0.22\n",
       "10         0.91      0.91\n",
       "11         0.17      0.18\n",
       "12         0.73      0.72\n",
       "13         0.30      0.42\n",
       "14         0.26      0.26\n",
       "15         0.67      0.69\n",
       "16         0.55      0.53\n",
       "17         0.82      0.83\n",
       "18         0.87      0.87\n",
       "19         0.62      0.63\n",
       "20         0.50      0.52\n",
       "21         0.73      0.74\n",
       "22         0.77      0.80\n",
       "23         0.61      0.59\n",
       "24         0.05      0.04\n",
       "25         0.69      0.72\n",
       "26         0.58      0.59\n",
       "27         0.08      0.13\n",
       "28         0.90      0.89\n",
       "29         0.33      0.33\n",
       "30         0.67      0.65\n",
       "31         0.96      0.96\n",
       "32         0.75      0.74\n",
       "33         0.21      0.18\n",
       "34         0.47      0.46\n",
       "35         0.57      0.60\n",
       "36         0.47      0.49\n",
       "37         0.58      0.57\n",
       "38         0.70      0.69\n",
       "39         0.68      0.64\n",
       "40         0.44      0.44\n",
       "41         0.53      0.55\n",
       "42         0.93      0.94\n",
       "43         0.32      0.29\n",
       "44         0.54      0.52\n",
       "45         0.74      0.73\n",
       "46         0.75      0.78\n",
       "47         0.90      0.89\n",
       "48         0.83      0.81\n",
       "49         0.18      0.17\n",
       "50         0.59      0.59\n",
       "51         0.72      0.74\n",
       "52         0.89      0.89\n",
       "53         0.73      0.73\n",
       "54         0.48      0.48\n",
       "55         0.80      0.79\n",
       "56         0.06      0.06\n",
       "57         0.84      0.83\n",
       "58         0.55      0.54\n",
       "59         0.68      0.69\n",
       "60         0.42      0.45\n",
       "61         0.92      0.92\n",
       "62         0.56      0.55\n",
       "63         0.39      0.39\n",
       "64         0.53      0.54\n",
       "65         0.75      0.76\n",
       "66         0.53      0.50\n",
       "67         0.02      0.02\n",
       "68         0.18      0.13\n",
       "69         0.48      0.50\n",
       "70         0.34      0.34\n",
       "71         0.64      0.62\n",
       "72         0.57      0.61\n",
       "73         0.58      0.57\n",
       "74         0.74      0.72\n",
       "75         0.41      0.41\n",
       "76         0.85      0.86\n",
       "77         0.47      0.47\n",
       "78         0.36      0.34\n",
       "79         0.67      0.68\n",
       "80         0.98      0.98\n",
       "81         0.70      0.75\n",
       "82         0.67      0.66\n",
       "83         0.56      0.55\n",
       "84         0.72      0.72\n",
       "85         0.95      0.95\n",
       "86         0.76      0.77\n",
       "87         0.19      0.21\n",
       "88         0.69      0.62\n",
       "89         0.84      0.85\n",
       "90         0.07      0.08\n",
       "91         0.27      0.25\n",
       "92         0.49      0.52\n",
       "93         0.88      0.91\n",
       "94         0.81      0.80\n",
       "95         0.52      0.49\n",
       "96         0.89      0.89\n",
       "97         0.81      0.79\n",
       "98         0.06      0.06\n",
       "99         0.35      0.33\n",
       "...         ...       ...\n",
       "14379      0.48      0.48\n",
       "14380      0.82      0.80\n",
       "14381      0.57      0.58\n",
       "14382      0.87      0.89\n",
       "14383      0.91      0.91\n",
       "14384      0.88      0.88\n",
       "14385      0.80      0.80\n",
       "14386      0.25      0.25\n",
       "14387      0.38      0.36\n",
       "14388      0.39      0.38\n",
       "14389      0.53      0.55\n",
       "14390      0.71      0.73\n",
       "14391      0.90      0.89\n",
       "14392      0.56      0.59\n",
       "14393      0.82      0.83\n",
       "14394      0.67      0.71\n",
       "14395      0.60      0.59\n",
       "14396      0.66      0.69\n",
       "14397      0.51      0.53\n",
       "14398      0.14      0.14\n",
       "14399      0.79      0.78\n",
       "14400      0.07      0.07\n",
       "14401      0.92      0.93\n",
       "14402      0.39      0.42\n",
       "14403      0.52      0.52\n",
       "14404      0.43      0.43\n",
       "14405      0.38      0.39\n",
       "14406      0.68      0.69\n",
       "14407      0.26      0.26\n",
       "14408      0.86      0.85\n",
       "14409      0.68      0.68\n",
       "14410      0.56      0.57\n",
       "14411      0.53      0.52\n",
       "14412      0.26      0.27\n",
       "14413      0.80      0.82\n",
       "14414      0.89      0.90\n",
       "14415      0.80      0.80\n",
       "14416      0.67      0.69\n",
       "14417      0.42      0.43\n",
       "14418      0.49      0.48\n",
       "14419      0.81      0.84\n",
       "14420      0.49      0.47\n",
       "14421      0.56      0.57\n",
       "14422      0.43      0.42\n",
       "14423      0.76      0.75\n",
       "14424      0.97      0.95\n",
       "14425      0.77      0.77\n",
       "14426      0.80      0.80\n",
       "14427      0.22      0.25\n",
       "14428      0.91      0.91\n",
       "14429      0.43      0.41\n",
       "14430      0.01      0.01\n",
       "14431      0.88      0.88\n",
       "14432      0.92      0.93\n",
       "14433      0.57      0.58\n",
       "14434      0.75      0.75\n",
       "14435      0.72      0.72\n",
       "14436      0.53      0.53\n",
       "14437      0.55      0.55\n",
       "14438      0.82      0.83\n",
       "14439      0.98      0.97\n",
       "14440      0.42      0.43\n",
       "14441      0.44      0.43\n",
       "14442      0.72      0.71\n",
       "14443      0.82      0.84\n",
       "14444      0.87      0.88\n",
       "14445      0.54      0.53\n",
       "14446      0.57      0.55\n",
       "14447      0.37      0.37\n",
       "14448      0.63      0.61\n",
       "14449      0.85      0.83\n",
       "14450      0.57      0.57\n",
       "14451      0.58      0.60\n",
       "14452      0.55      0.55\n",
       "14453      0.50      0.50\n",
       "14454      0.74      0.72\n",
       "14455      0.76      0.79\n",
       "14456      0.70      0.71\n",
       "14457      0.63      0.63\n",
       "14458      0.71      0.73\n",
       "14459      0.32      0.34\n",
       "14460      0.64      0.63\n",
       "14461      0.50      0.51\n",
       "14462      0.88      0.88\n",
       "14463      0.46      0.46\n",
       "14464      0.92      0.92\n",
       "14465      0.78      0.78\n",
       "14466      0.42      0.41\n",
       "14467      0.72      0.73\n",
       "14468      0.20      0.19\n",
       "14469      0.29      0.26\n",
       "14470      0.47      0.48\n",
       "14471      0.20      0.19\n",
       "14472      0.49      0.50\n",
       "14473      0.41      0.40\n",
       "14474      0.93      0.92\n",
       "14475      0.13      0.24\n",
       "14476      0.52      0.51\n",
       "14477      0.76      0.76\n",
       "14478      0.38      0.36\n",
       "\n",
       "[57915 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[['glmpreds', 'gampreds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slowevents_prop',\n",
       " 'medevents_prop',\n",
       " 'memmonthscbd',\n",
       " 'r3',\n",
       " 'region',\n",
       " 'knnpreds',\n",
       " 'r2',\n",
       " 'r1',\n",
       " 'gampreds',\n",
       " 'r3r1',\n",
       " 'allgames_change',\n",
       " 'rf2preds',\n",
       " 'r.quick',\n",
       " 'r3r2',\n",
       " 'glmpreds',\n",
       " 'nn2preds',\n",
       " 'rf1preds',\n",
       " 'nn1preds',\n",
       " 'svmpreds',\n",
       " 'gbpolypreds']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(top_names[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_top = df_all[list(top_names[-20:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52640\n",
      "Test set error = 0.52531\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51602\n",
      "Test set error = 0.53300\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50094\n",
      "Test set error = 0.54009\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52306\n",
      "Test set error = 0.54076\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51556\n",
      "Test set error = 0.53235\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50253\n",
      "Test set error = 0.53526\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52150\n",
      "Test set error = 0.54506\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51483\n",
      "Test set error = 0.54044\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50520\n",
      "Test set error = 0.53937\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52244\n",
      "Test set error = 0.54149\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51485\n",
      "Test set error = 0.53707\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49944\n",
      "Test set error = 0.54100\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52245\n",
      "Test set error = 0.53973\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51485\n",
      "Test set error = 0.53850\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50247\n",
      "Test set error = 0.53373\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52471\n",
      "Test set error = 0.53068\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51442\n",
      "Test set error = 0.53838\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50306\n",
      "Test set error = 0.53844\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52297\n",
      "Test set error = 0.53603\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51345\n",
      "Test set error = 0.53830\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50078\n",
      "Test set error = 0.53641\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52463\n",
      "Test set error = 0.53157\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51300\n",
      "Test set error = 0.53987\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50337\n",
      "Test set error = 0.53408\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52268\n",
      "Test set error = 0.53717\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51533\n",
      "Test set error = 0.53324\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50367\n",
      "Test set error = 0.53835\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51164\n",
      "Test set error = 0.54311\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49342\n",
      "Test set error = 0.53639\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46818\n",
      "Test set error = 0.54124\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51255\n",
      "Test set error = 0.53590\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49447\n",
      "Test set error = 0.53959\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47256\n",
      "Test set error = 0.53835\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51284\n",
      "Test set error = 0.53580\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49654\n",
      "Test set error = 0.54043\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47596\n",
      "Test set error = 0.53747\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51122\n",
      "Test set error = 0.54023\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49042\n",
      "Test set error = 0.54629\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46611\n",
      "Test set error = 0.53665\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51340\n",
      "Test set error = 0.53530\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49622\n",
      "Test set error = 0.53372\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47300\n",
      "Test set error = 0.53189\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51223\n",
      "Test set error = 0.53814\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49522\n",
      "Test set error = 0.53514\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47225\n",
      "Test set error = 0.54616\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51086\n",
      "Test set error = 0.53862\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49019\n",
      "Test set error = 0.54205\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46897\n",
      "Test set error = 0.54007\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50961\n",
      "Test set error = 0.54829\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49232\n",
      "Test set error = 0.53985\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47468\n",
      "Test set error = 0.53135\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51439\n",
      "Test set error = 0.53106\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49410\n",
      "Test set error = 0.54431\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47265\n",
      "Test set error = 0.54028\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49500\n",
      "Test set error = 0.53502\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46282\n",
      "Test set error = 0.53844\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41901\n",
      "Test set error = 0.54452\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49463\n",
      "Test set error = 0.54176\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46252\n",
      "Test set error = 0.54494\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42563\n",
      "Test set error = 0.54429\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49559\n",
      "Test set error = 0.54357\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46806\n",
      "Test set error = 0.54328\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43074\n",
      "Test set error = 0.54693\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49356\n",
      "Test set error = 0.53357\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45887\n",
      "Test set error = 0.54377\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.41787\n",
      "Test set error = 0.54972\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49370\n",
      "Test set error = 0.54119\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46303\n",
      "Test set error = 0.54527\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42307\n",
      "Test set error = 0.55671\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49427\n",
      "Test set error = 0.54609\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46537\n",
      "Test set error = 0.55022\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42855\n",
      "Test set error = 0.54833\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49145\n",
      "Test set error = 0.54319\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.45965\n",
      "Test set error = 0.54453\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42390\n",
      "Test set error = 0.54028\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49414\n",
      "Test set error = 0.53956\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46072\n",
      "Test set error = 0.54300\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42652\n",
      "Test set error = 0.54575\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49475\n",
      "Test set error = 0.54056\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46501\n",
      "Test set error = 0.53830\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43222\n",
      "Test set error = 0.54027\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "estimators = [100, 250, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                model_top = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d, random_state=1004)\n",
    "                p2.get_pred_np(model_top, df_all_top, train_y, 'RFBoost', track_dict=None, \n",
    "                            test_idx=test_idx, train_size=0.8, columns=None, parameters=None, \n",
    "                            score_func='log_loss', predict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
    "              min_samples_leaf=25, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.52640\n",
    "Test set error = 0.52531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stacked20 = GradientBoostingClassifier(n_estimators=100, max_features=0.6, min_samples_leaf=25, max_depth=3,\n",
    "                                             random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_stacked20 = p2.fit_and_predict(model_stacked20, df_all_top, train_y, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14479,)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_stacked20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52548636549050265"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y, model_stacked20.predict_proba(df_all_top[:test_idx])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p2.write_to_file('predictions/stacking_gbx_top20.csv', pred_stacked20, test_ids)\n",
    "\n",
    "# actual 0.47586477423949575\n",
    "# validation 0.52677\n",
    "# Kaggle 0.53684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_top15 = df_all[list(top_names[-15:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52272\n",
      "Test set error = 0.54226\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51597\n",
      "Test set error = 0.53875\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50371\n",
      "Test set error = 0.53680\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52330\n",
      "Test set error = 0.53744\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51659\n",
      "Test set error = 0.53720\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50579\n",
      "Test set error = 0.53111\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52349\n",
      "Test set error = 0.53831\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51781\n",
      "Test set error = 0.53593\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50612\n",
      "Test set error = 0.53958\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52431\n",
      "Test set error = 0.53479\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51350\n",
      "Test set error = 0.54596\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50312\n",
      "Test set error = 0.53533\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52577\n",
      "Test set error = 0.52816\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51720\n",
      "Test set error = 0.53157\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50385\n",
      "Test set error = 0.54117\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52300\n",
      "Test set error = 0.54051\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51752\n",
      "Test set error = 0.53393\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50290\n",
      "Test set error = 0.54553\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52412\n",
      "Test set error = 0.53410\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51739\n",
      "Test set error = 0.53146\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50393\n",
      "Test set error = 0.53173\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52440\n",
      "Test set error = 0.53464\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51738\n",
      "Test set error = 0.53243\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50301\n",
      "Test set error = 0.54171\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.52511\n",
      "Test set error = 0.53196\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51799\n",
      "Test set error = 0.53564\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50506\n",
      "Test set error = 0.54178\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51457\n",
      "Test set error = 0.53458\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49648\n",
      "Test set error = 0.53676\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47536\n",
      "Test set error = 0.53433\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51411\n",
      "Test set error = 0.53684\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49491\n",
      "Test set error = 0.54690\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47497\n",
      "Test set error = 0.54650\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51603\n",
      "Test set error = 0.53101\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50026\n",
      "Test set error = 0.53292\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47507\n",
      "Test set error = 0.54514\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51027\n",
      "Test set error = 0.54785\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49320\n",
      "Test set error = 0.54163\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47143\n",
      "Test set error = 0.54553\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51288\n",
      "Test set error = 0.54147\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49523\n",
      "Test set error = 0.54302\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47696\n",
      "Test set error = 0.53732\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51329\n",
      "Test set error = 0.53983\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49659\n",
      "Test set error = 0.54772\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.48048\n",
      "Test set error = 0.53417\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51193\n",
      "Test set error = 0.54134\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49611\n",
      "Test set error = 0.53662\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47035\n",
      "Test set error = 0.53743\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51016\n",
      "Test set error = 0.54701\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49389\n",
      "Test set error = 0.54282\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47678\n",
      "Test set error = 0.53986\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.51282\n",
      "Test set error = 0.53905\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49993\n",
      "Test set error = 0.53341\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=250,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47939\n",
      "Test set error = 0.53485\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49660\n",
      "Test set error = 0.53939\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46336\n",
      "Test set error = 0.54681\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42857\n",
      "Test set error = 0.54027\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49815\n",
      "Test set error = 0.53612\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47127\n",
      "Test set error = 0.53487\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43373\n",
      "Test set error = 0.54362\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49765\n",
      "Test set error = 0.54198\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47100\n",
      "Test set error = 0.53973\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.6, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43907\n",
      "Test set error = 0.54964\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49684\n",
      "Test set error = 0.53366\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46611\n",
      "Test set error = 0.53857\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42831\n",
      "Test set error = 0.55389\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49462\n",
      "Test set error = 0.54538\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46360\n",
      "Test set error = 0.55367\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43535\n",
      "Test set error = 0.53472\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49594\n",
      "Test set error = 0.54405\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47097\n",
      "Test set error = 0.54392\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.75, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43957\n",
      "Test set error = 0.54045\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49410\n",
      "Test set error = 0.54378\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46038\n",
      "Test set error = 0.55076\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=25, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.42893\n",
      "Test set error = 0.54405\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.49462\n",
      "Test set error = 0.54606\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.46598\n",
      "Test set error = 0.54355\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=35, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43465\n",
      "Test set error = 0.54102\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.50001\n",
      "Test set error = 0.52700\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=4, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.47245\n",
      "Test set error = 0.54474\n",
      "----------\n",
      "############\n",
      "############\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=5, max_features=0.9, max_leaf_nodes=None,\n",
      "              min_samples_leaf=45, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------\n",
      "Training set error = 0.43807\n",
      "Test set error = 0.54906\n",
      "----------\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "estimators = [100, 250, 500] # default = 10\n",
    "features = [0.6, 0.75, 0.9] # default = 'sqrt'\n",
    "samples = [25, 35, 45] # default = 1\n",
    "max_depth = [3, 4, 5]\n",
    "\n",
    "for e in estimators:\n",
    "    for f in features: \n",
    "        for s in samples: \n",
    "            for d in max_depth: \n",
    "                model_top15 = GradientBoostingClassifier(n_estimators=e, max_features=f, min_samples_leaf=s, \n",
    "                                                   max_depth=d, random_state=1004)\n",
    "                p2.get_pred_np(model_top15, df_all_top15, train_y, 'RFBoost', track_dict=None, \n",
    "                            test_idx=test_idx, train_size=0.8, columns=None, parameters=None, \n",
    "                            score_func='log_loss', predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.9, max_leaf_nodes=None,\n",
    "              min_samples_leaf=45, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.50001\n",
    "Test set error = 0.52700\n",
    "----------\n",
    "\n",
    "\n",
    "\n",
    "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=0.75, max_leaf_nodes=None,\n",
    "              min_samples_leaf=35, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1004, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "----------\n",
    "Training set error = 0.52577\n",
    "Test set error = 0.52816\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
